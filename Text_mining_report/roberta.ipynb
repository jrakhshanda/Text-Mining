{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekcMUspsNFY2",
        "outputId": "720a48d4-1e27-4094-8ff9-395274c77f39"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVGoYWYBP9gs"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install tokenizers\r\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\r\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\r\n",
        "#!export XLA_USE_BF16=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDDlFXNGvhtc"
      },
      "source": [
        "#df, test = model_selection.train_test_split(df_train,test_size=0.1)\r\n",
        "#test.to_csv('/content/drive/MyDrive/BERT_files/test_fold.csv')\r\n",
        "\r\n",
        "#df[\"kfold\"] = -1\r\n",
        "#df = df.sample(frac=1).reset_index(drop=True)\r\n",
        "\r\n",
        "#kf = model_selection.StratifiedKFold(n_splits=5)\r\n",
        "\r\n",
        "#for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\r\n",
        "#    print(len(trn_), len(val_))\r\n",
        "#    df.loc[val_, 'kfold'] = fold\r\n",
        "\r\n",
        "#df.to_csv(\"/content/drive/MyDrive/RoBERTa_files/train_fold.csv\", index=False)\r\n",
        "#test.to_csv(\"/content/drive/MyDrive/RoBERTa_files/test_fold.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9gYGcLNTTz"
      },
      "source": [
        "import os\r\n",
        "import string\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn import model_selection\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from transformers import *\r\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
        "import tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWxMIPZy5W5T",
        "outputId": "570d67f8-f7d9-4227-f058-7265e813138a"
      },
      "source": [
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7q1nawUPMqT"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgd78eqmNbzc"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\r\n",
        "class config:\r\n",
        "    TRAIN_BATCH_SIZE = 64\r\n",
        "    VALID_BATCH_SIZE = 32\r\n",
        "    EPOCHS = 3\r\n",
        "    PATH = '/content/drive/MyDrive/RoBERTa_files'\r\n",
        "    TRAINING_FILE = pd.read_csv(\"/content/drive/MyDrive/RoBERTa_files/train_folds.csv\",keep_default_na=False)\r\n",
        "    TEST_FILE =  pd.read_csv(\"/content/drive/MyDrive/RoBERTa_files/test.csv\")\r\n",
        "    MAX_LEN = 128\r\n",
        "    TOKENIZER = ByteLevelBPETokenizer(f\"{PATH}/vocab.json\",\r\n",
        "                                      f\"{PATH}/merges.txt\",\r\n",
        "                                      lowercase=True, add_prefix_space=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOXQ5K2XA-l"
      },
      "source": [
        "# Processing of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-qngeJ3Xahc"
      },
      "source": [
        "def process_data(text, selected_text, sentiment, tokenizer=config.TOKENIZER, max_len=config.MAX_LEN):\r\n",
        "\r\n",
        "    text = \" \" + \" \".join(str(text).split())\r\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\r\n",
        "\r\n",
        "    len_st = len(selected_text) - 1\r\n",
        "    idx1 = idx2 = None\r\n",
        "    for ind in (i for i, e in enumerate(text) if e == selected_text[0]):\r\n",
        "      if text[ind: ind+len_st] == selected_text:\r\n",
        "        idx1 = ind\r\n",
        "        idx2 = ind + len_st - 1\r\n",
        "        break\r\n",
        "\r\n",
        "    char_targets = [0] * len(text)\r\n",
        "\r\n",
        "    if idx1!=None and idx2!=None:\r\n",
        "        for i in range(idx1, idx2+1):\r\n",
        "            char_targets[i] = 1\r\n",
        "    else:\r\n",
        "        char_targets = [1] * len(text)\r\n",
        "\r\n",
        "    # encoding using pretrained tokenizer\r\n",
        "    tok_text = tokenizer.encode(text)\r\n",
        "    ids_orig = tok_text.ids\r\n",
        "    offsets = tok_text.offsets\r\n",
        "\r\n",
        "    # getting indexes of tokens containing character in selected_text\r\n",
        "    target_idx = []\r\n",
        "    for i, (offset1, offset2) in enumerate(offsets):\r\n",
        "        if sum(char_targets[offset1: offset2])>0:\r\n",
        "            target_idx.append(i)\r\n",
        "\r\n",
        "    # we just need the offset indices of the start and end tokens as we are using \r\n",
        "    targets_start = target_idx[0]\r\n",
        "    targets_end = target_idx[-1]\r\n",
        "\r\n",
        "    # token ids of sentiment as present in our vocab hard coded here\r\n",
        "    sentiment_ids = {\r\n",
        "        'positive':1313,                    # tokenizer.encode('positive').ids\r\n",
        "        'negative':2430,                    # tokenizer.encode('negative').ids\r\n",
        "        'neutral':7974                     # tokenizer.encode('neutral').ids\r\n",
        "    }\r\n",
        "\r\n",
        "    # adding special tokens\r\n",
        "    input_ids = [0] + ids_orig + [2] # adding a cls token at start two SEP tokens at the end of sentiment \r\n",
        "    token_type_ids = [0, 0] + [0] * len(ids_orig) # since Roberta does not need token type ids for training\r\n",
        "    attention_mask = [0] + [1] * len(ids_orig) + [0]\r\n",
        "    offsets = [(0, 0)] * 2 + offsets # obtaining offsets of onnly tweet and adding zero for sentiments\r\n",
        "    targets_start += 2 # adding CLS sentiment and two SEP tokens\r\n",
        "    targets_end += 2\r\n",
        "\r\n",
        "    # padding\r\n",
        "    padding_len = max_len - len(input_ids)\r\n",
        "    if padding_len>0:\r\n",
        "        input_ids = input_ids + [1] * padding_len\r\n",
        "        attention_mask = attention_mask + [0] * padding_len\r\n",
        "        token_type_ids = token_type_ids + [0] * padding_len\r\n",
        "        offsets = offsets + [(0, 0)] * padding_len\r\n",
        "\r\n",
        "    return {\r\n",
        "        'ids': torch.tensor(input_ids,dtype=torch.long),\r\n",
        "        'attention_mask': torch.tensor(attention_mask,dtype=torch.long),\r\n",
        "        'token_type_ids':torch.tensor(token_type_ids,dtype=torch.long),\r\n",
        "        'targets_start': torch.tensor(targets_start,dtype=torch.long),\r\n",
        "        'targets_end':  torch.tensor(targets_end,dtype=torch.long),\r\n",
        "        'offsets': torch.tensor(offsets,dtype=torch.long),\r\n",
        "        'text': text,\r\n",
        "        'selected_text': selected_text,\r\n",
        "        'sentiment': sentiment\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW51NizeXrjl"
      },
      "source": [
        "class TextDataset(Dataset):\r\n",
        "    def __init__(self, text, sentiment, selected_text):\r\n",
        "        self.text = text\r\n",
        "        self.sentiment = sentiment\r\n",
        "        self.selected_text = selected_text\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.text)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        # processing data\r\n",
        "        data = process_data(\r\n",
        "            self.text[item], \r\n",
        "            self.selected_text[item], \r\n",
        "            self.sentiment[item]\r\n",
        "        )\r\n",
        "        # returning tensors\r\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN2ApvivZSEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32db8caf-dc72-442f-e2c5-4fdfb683c45e"
      },
      "source": [
        "#import pdb\r\n",
        "#pdb.set_trace()\r\n",
        "df = config.TRAINING_FILE.reset_index(drop=True)\r\n",
        "if __name__== \"__main__\":\r\n",
        "  dset = TextDataset(text = df.text.values,\r\n",
        "                      selected_text =df.selected_text.values,sentiment = df.sentiment.values)\r\n",
        "  print(dset[500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ids': tensor([    0,   662,   358,   134,   328, 30385,    23,  2054,   640, 40743,\n",
            "         6423,     4,   175,    73, 16593,   438,   306,   298,   571, 11134,\n",
            "         4607,  2841,  1916,  6184,   359,  1629,  1244,  4085, 25782,     7,\n",
            "         6605, 36636,  4063, 12846,   213,  1649,    24,    66,     2,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]), 'targets_start': tensor(2), 'targets_end': tensor(38), 'offsets': tensor([[  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   8],\n",
            "        [  8,  14],\n",
            "        [ 14,  15],\n",
            "        [ 15,  16],\n",
            "        [ 16,  25],\n",
            "        [ 25,  28],\n",
            "        [ 28,  33],\n",
            "        [ 33,  36],\n",
            "        [ 36,  40],\n",
            "        [ 40,  43],\n",
            "        [ 43,  44],\n",
            "        [ 44,  47],\n",
            "        [ 47,  48],\n",
            "        [ 48,  50],\n",
            "        [ 50,  51],\n",
            "        [ 51,  52],\n",
            "        [ 52,  53],\n",
            "        [ 53,  54],\n",
            "        [ 54,  58],\n",
            "        [ 58,  62],\n",
            "        [ 62,  65],\n",
            "        [ 65,  67],\n",
            "        [ 67,  75],\n",
            "        [ 75,  77],\n",
            "        [ 77,  78],\n",
            "        [ 78,  80],\n",
            "        [ 80,  85],\n",
            "        [ 85,  89],\n",
            "        [ 89,  92],\n",
            "        [ 92,  96],\n",
            "        [ 96,  99],\n",
            "        [ 99, 102],\n",
            "        [102, 104],\n",
            "        [104, 107],\n",
            "        [107, 113],\n",
            "        [113, 116],\n",
            "        [116, 120],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0],\n",
            "        [  0,   0]]), 'text': ' MORNING EVERY1! Giveaway at http://tinyurl.com/dhc4hg Mod Kid Emma pattern &$25 GiftCert to HipFabric!! GO check it out', 'selected_text': ' MORNING EVERY1! Giveaway at http://tinyurl.com/dhc4hg Mod Kid Emma pattern &$25 GiftCert to HipFabric!! GO check it out', 'sentiment': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYBH6-oPn77R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1621e2-a29a-4a7a-b234-bfbc894cf790"
      },
      "source": [
        "len(dset[1]['token_type_ids'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwNpnriG8GZv"
      },
      "source": [
        "Now we’ll create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcCUgOU8PRA2"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eadEGBNw76"
      },
      "source": [
        "class TextModel(BertPreTrainedModel):\r\n",
        "    def __init__(self,conf):\r\n",
        "        super(TextModel, self).__init__(conf)\r\n",
        "\r\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\",config = conf)\r\n",
        "        self.drop_out = nn.Dropout(0.1)\r\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\r\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\r\n",
        "        # this is to initialize the weights of the matrix that would convert \r\n",
        "        # (batch_size, max_len, 2*768) to (batch_size, max_len, 1) with std=0.02 \r\n",
        "    \r\n",
        "    def forward(self, ids, attention_mask, token_type_ids):\r\n",
        "        _, _, output = self.roberta(ids,\r\n",
        "                                    attention_mask = attention_mask,\r\n",
        "                                    token_type_ids=token_type_ids).to_tuple()\r\n",
        "        \r\n",
        "        # out dim = (12, batch_size, max_len, 768)\r\n",
        "        # 12 denotes the 12 hidden layers of roberta\r\n",
        "\r\n",
        "        output = torch.cat((output[-1], output[-2]), dim=-1)\r\n",
        "        # output dim = (batch_size, max_len, 2*768)\r\n",
        "        \r\n",
        "        output = self.drop_out(output)\r\n",
        "        logits = self.l0(output)\r\n",
        "        # logits dim -> (batch_size, max_len, 2)\r\n",
        "\r\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len, 1)\r\n",
        "\r\n",
        "        start_logits = start_logits.squeeze(-1)\r\n",
        "        end_logits = end_logits.squeeze(-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len)\r\n",
        "\r\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmkzSVgdHr0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e98fe4-96e3-4712-a1a5-cd64dd881b69"
      },
      "source": [
        "conf = RobertaConfig.from_pretrained(\"roberta-base\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "model = TextModel(conf)\r\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextModel(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop_out): Dropout(p=0.1, inplace=False)\n",
              "  (l0): Linear(in_features=1536, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lU4SNuJLsiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3aaedee-31cd-4d0e-a7fb-a45de18effd7"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\r\n",
        "params = list(model.named_parameters())\r\n",
        "print('The RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\r\n",
        "print('==== Embedding Layer ====\\n')\r\n",
        "for p in params[0:5]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "print('\\n==== First Transformer ====\\n')\r\n",
        "for p in params[5:21]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "print('\\n==== Output Layer ====\\n')\r\n",
        "for p in params[-4:]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The RoBERTa model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "roberta.pooler.dense.weight                               (768, 768)\n",
            "roberta.pooler.dense.bias                                     (768,)\n",
            "l0.weight                                                  (2, 1536)\n",
            "l0.bias                                                         (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZjMaPSEYTvi"
      },
      "source": [
        "# loss function. Play around with it and see what works best\r\n",
        "def loss_fn(output_start, output_end, targets_start, targets_end,device):\r\n",
        "  loss = nn.CrossEntropyLoss().to(device)\r\n",
        "  l1 = loss(output_start,targets_start)\r\n",
        "  l2 = loss(output_end,targets_end)\r\n",
        "  return l1 + l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fABYRm3GBTuC"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n",
        "t0 = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpBxAHPlVzDB"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler=None):     \r\n",
        "  model.train()\r\n",
        "  train_loss = []\r\n",
        "  for bi, batch in enumerate(data_loader):    \r\n",
        "    ids = batch['ids'].to(device, dtype=torch.long)\r\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "    attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "    targets_start = batch['targets_start'].to(device, dtype=torch.long)\r\n",
        "    targets_end = batch['targets_end'].to(device, dtype=torch.long)\r\n",
        "    \r\n",
        "      \r\n",
        "    model.zero_grad()\r\n",
        "      \r\n",
        "    output_start,output_end = model(ids,\r\n",
        "                     attention_mask = attention_mask,\r\n",
        "                     token_type_ids = token_type_ids) \r\n",
        "      \r\n",
        "    # calculating loss\r\n",
        "    loss = loss_fn(output_start, output_end, targets_start, targets_end, device)\r\n",
        "\r\n",
        "    # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\r\n",
        "    train_loss.append(loss.item())\r\n",
        "\r\n",
        "    # Perform a backward pass to calculate the gradients.\r\n",
        "    loss.backward()\r\n",
        "    # modified based on their gradients, the learning rate, etc.\r\n",
        "    optimizer.step()\r\n",
        "    # Update the learning rate.\r\n",
        "    scheduler.step()\r\n",
        "  \r\n",
        "  avg_train_loss = np.mean(train_loss)\r\n",
        "  \r\n",
        "  print(\"\")\r\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "  print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "  \r\n",
        "  return  avg_train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI6ea68oYSPq"
      },
      "source": [
        "# jaccard function as mentioned in evaluation section of the contest\r\n",
        "def jaccard_metric(text,\r\n",
        "                   selected_text,\r\n",
        "                   sentiment,\r\n",
        "                   offsets,\r\n",
        "                   start_idx,\r\n",
        "                   end_idx): \r\n",
        "  \r\n",
        "  if end_idx < start_idx:\r\n",
        "    end_idx = start_idx\r\n",
        "    \r\n",
        "  pred  = \"\"\r\n",
        "  for idx in range(start_idx, end_idx + 1):\r\n",
        "    pred += text[offsets[idx][0]: offsets[idx][1]]\r\n",
        "    if (idx+1) < len(offsets) and offsets[idx][1] < offsets[idx+1][0]:\r\n",
        "      pred += \" \"\r\n",
        "\r\n",
        "  if len(text.split()) < 2 or sentiment=='neutral':\r\n",
        "    pred = text\r\n",
        "    \r\n",
        "  a = set(selected_text.lower().split()) \r\n",
        "  b = set(pred.lower().split())\r\n",
        "  c = a.intersection(b)\r\n",
        "  jacc = float(len(c)) / (len(a) + len(b) - len(c))\r\n",
        "\r\n",
        "  return jacc, pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o3V2nFgYHy9"
      },
      "source": [
        "def evaluate_fn(data_loader,model, device):  \r\n",
        "  predicted_text = []\r\n",
        "  jaccard = []\r\n",
        "  model.eval()\r\n",
        "  with torch.no_grad():\r\n",
        "    for bi, batch in enumerate(data_loader):\r\n",
        "      ids = batch[\"ids\"].to(device, dtype=torch.long)\r\n",
        "      token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "      attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "      targets_start = batch['targets_start'].to(device, dtype=torch.long)\r\n",
        "      targets_end = batch['targets_end'].to(device, dtype=torch.long)\r\n",
        "      offsets = batch['offsets'].cpu().numpy()\r\n",
        "      text = batch['text']\r\n",
        "      selected_text = batch['selected_text']\r\n",
        "      sentiment = batch['sentiment']\r\n",
        "\r\n",
        "      output_start, output_end = model(ids,\r\n",
        "                                       attention_mask=attention_mask,\r\n",
        "                                       token_type_ids=token_type_ids)\r\n",
        "      \r\n",
        "      output_start = torch.softmax(output_start, dim=1).cpu().detach().numpy()\r\n",
        "      output_end = torch.softmax(output_end, dim=1).cpu().detach().numpy()\r\n",
        "\r\n",
        "      for px, tweet in enumerate(text):\r\n",
        "\r\n",
        "        jacc, pred = jaccard_metric(tweet,\r\n",
        "                                    selected_text[px],\r\n",
        "                                    sentiment = sentiment[px],\r\n",
        "                                    offsets = offsets[px,:],\r\n",
        "                                    start_idx = np.argmax(output_start[px,]),\r\n",
        "                                    end_idx = np.argmax(output_end[px,]))\r\n",
        "\r\n",
        "        predicted_text.append(pred)  \r\n",
        "        jaccard.append(jacc)\r\n",
        "  print(\"  Average jaccard similarity on validation data: {0:.2f}\".format(np.mean(jaccard)))\r\n",
        "  print(\"  validation took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "  return np.mean(jaccard), predicted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA-d3nRp0mWN"
      },
      "source": [
        "def run(fold):\r\n",
        "  df = config.TRAINING_FILE #.sample(1000)\r\n",
        "  df = df.reset_index(drop=True)\r\n",
        "  train =  df[df.kfold != fold]\r\n",
        "  valid = df[df.kfold == fold]\r\n",
        "\r\n",
        "  train_dataset = TextDataset(text = train.text.values,\r\n",
        "                              selected_text = train.selected_text.values,\r\n",
        "                              sentiment = train.sentiment.values)\r\n",
        "\r\n",
        "  train_dataloader = DataLoader(train_dataset,\r\n",
        "                              batch_size = config.TRAIN_BATCH_SIZE,\r\n",
        "                              shuffle = False,\r\n",
        "                              num_workers=1)\r\n",
        "\r\n",
        "  valid_dataset = TextDataset(text = valid.text.values,\r\n",
        "                              selected_text = valid.selected_text.values,\r\n",
        "                              sentiment = valid.sentiment.values)\r\n",
        "\r\n",
        "  valid_dataloader = DataLoader(valid_dataset,\r\n",
        "                                batch_size = config.VALID_BATCH_SIZE,\r\n",
        "                                shuffle = False,\r\n",
        "                                num_workers=1)\r\n",
        "\r\n",
        "  conf = RobertaConfig.from_pretrained(\"roberta-base\")\r\n",
        "  conf.output_hidden_states = True\r\n",
        "  model = TextModel(conf)\r\n",
        "  model.to(device)\r\n",
        "  model = nn.DataParallel(model)\r\n",
        "\r\n",
        "  param_optimizer = list(model.named_parameters())\r\n",
        "  no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\r\n",
        "  optimizer_parameters = [{\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\"weight_decay\": 0.01},\r\n",
        "                          {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\"weight_decay\": 0}]\r\n",
        "\r\n",
        "  num_train_steps = len(train) / config.TRAIN_BATCH_SIZE * config.EPOCHS\r\n",
        "\r\n",
        "  optimizer = AdamW(optimizer_parameters, lr = 4e-5,weight_decay=0.01)\r\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\r\n",
        "\r\n",
        "  train_loss = []\r\n",
        "  jaccards = []\r\n",
        "  best_jaccard = 0\r\n",
        "  for epoch in range(0, config.EPOCHS):\r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch+1, config.EPOCHS))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "  \r\n",
        "    avg_train_loss = train_fn(train_dataloader,model,optimizer,device,scheduler)\r\n",
        "    train_loss.append(avg_train_loss)\r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "    t0 = time.time()\r\n",
        "    jacc,_ = evaluate_fn(valid_dataloader, model, device)\r\n",
        "    jaccards.append(jacc)\r\n",
        "\r\n",
        "    if jacc > best_jaccard:\r\n",
        "      best_jaccard = jacc\r\n",
        "      print('saving model')\r\n",
        "      torch.save(model.state_dict(), '/content/drive/MyDrive/RoBERTa_files/jaccs7.pth')    \r\n",
        "  \r\n",
        "  torch.save(model.state_dict(), '/content/drive/MyDrive/RoBERTa_files/model7.pth')\r\n",
        "\r\n",
        "  return jaccards, train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8D47foPU0B3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b69877f5-87a2-4629-8a50-3ccd9b2ad01b"
      },
      "source": [
        "print('running zero fold')\r\n",
        "jaccs_fold0, train_loss_fold0 = run(fold=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running zero fold\n",
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:02:42\n",
            "\n",
            "Running Validation...\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 0:02:50\n",
            "saving model\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:05:31\n",
            "\n",
            "Running Validation...\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 0:05:39\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:08:18\n",
            "\n",
            "Running Validation...\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 0:08:26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYY-MC5VXhf"
      },
      "source": [
        " plt.plot(train_loss_fold0)\r\n",
        "plt.ylabel('Training loss on fold 0')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PgtlBbEVFgW"
      },
      "source": [
        "print('running first fold')\r\n",
        "jaccs_fold1, train_loss_fold1 = run(fold=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC6b04InVXr0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485c91bb-1668-4090-e7ed-f6609d21290e"
      },
      "source": [
        "print('running second fold')\r\n",
        "jaccs_fold2, train_loss_fold2 = run(fold=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running second fold\n",
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 1:43:03\n",
            "\n",
            "Running Validation...\n",
            "  Average loss on validation data: 0.00\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 1:43:16\n",
            "saving model\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 1:46:38\n",
            "\n",
            "Running Validation...\n",
            "  Average loss on validation data: 0.00\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 1:46:50\n",
            "\n",
            "======== Epoch 3 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 1:50:11\n",
            "\n",
            "Running Validation...\n",
            "  Average loss on validation data: 0.00\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 1:50:23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wwBfvKnssji"
      },
      "source": [
        "print('running third fold')\r\n",
        "jaccs_fold3, train_loss_fold3 = run(fold=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av4tXHnONiql"
      },
      "source": [
        "print('running fourth fold')\r\n",
        "jaccs_fold4, train_loss_fold4 = run(fold=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soaE0F_nTaGo"
      },
      "source": [
        "print('running 5th fold')\r\n",
        "jaccs_fold5, train_loss_fold5 = run(fold=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n95pd7d5Tqh0"
      },
      "source": [
        "print('running fourth fold')\r\n",
        "jaccs_fold6, train_loss_fold6 = run(fold=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpyIXbqdub5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "875d792e-b4ae-470c-aa98-535f04cfa151"
      },
      "source": [
        "print('running fourth fold')\r\n",
        "jaccs_fold6, train_loss_fold6 = run(fold=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running fourth fold\n",
            "\n",
            "======== Epoch 1 / 3 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:12:48\n",
            "\n",
            "Running Validation...\n",
            "  Average jaccard similarity on validation data: 0.59\n",
            "  validation took: 0:12:56\n",
            "saving model\n",
            "\n",
            "======== Epoch 2 / 3 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-69e8b8e4df7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'running fourth fold'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjaccs_fold6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_fold6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-8c7c27ac39b7>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# ========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f65d5524dc01>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3rw1_eJOWlp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d43a24e-a330-4102-fa1d-bdc266eb3f36"
      },
      "source": [
        "jaccs = []\r\n",
        "losses = []\r\n",
        "jaccs.append(np.mean(np.mean(jaccs_fold0)))\r\n",
        "jaccs.append(np.mean(jaccs_fold1))\r\n",
        "#jaccs.append(np.mean(jaccs_fold2))\r\n",
        "#jaccs.append(np.mean(jaccs_fold3))\r\n",
        "#jaccs.append(np.mean(jaccs_fold4))\r\n",
        "\r\n",
        "losses.append(np.mean(np.mean(train_loss_fold0)))\r\n",
        "losses.append(np.mean(train_loss_fold1))\r\n",
        "#losses.append(np.mean(train_loss_fold2))\r\n",
        "#losses.append(np.mean(jaccs_fold3))\r\n",
        "#losses.append(np.mean(jaccs_fold4))\r\n",
        "jaccs\r\n",
        "losses"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04989576670227894, 0.04995759922196865]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWj3bFf3OYux",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "14195291-9376-48ff-e583-0823cd7cc1a0"
      },
      "source": [
        "print('Jaccard similaritt and train loss average per fold')\r\n",
        "data = {'jaccard': jaccs, 'train_loss': losses}\r\n",
        "metrics = pd.DataFrame(data)\r\n",
        "metrics.to_csv('/content/drive/MyDrive/roberta_metrics.csv')\r\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jaccard similaritt and train loss average per fold\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jaccard</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.589685</td>\n",
              "      <td>0.049896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.589087</td>\n",
              "      <td>0.049958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    jaccard  train_loss\n",
              "0  0.589685    0.049896\n",
              "1  0.589087    0.049958"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ9nncAVOsq6"
      },
      "source": [
        "## Testing the final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3ZdR-NmOrPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49773dc0-ce74-4872-b255-c55470b117a3"
      },
      "source": [
        "conf = RobertaConfig.from_pretrained(\"roberta-base\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "\r\n",
        "model0 = TextModel(conf)\r\n",
        "model0.to(device)\r\n",
        "model0 = nn.DataParallel(model0)\r\n",
        "model0.load_state_dict(torch.load(\"/content/drive/MyDrive/RoBERTa_files/model0.pth\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b8uXo0gnLYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5d5131-a8c6-4984-f7c0-ec3f0936d7a9"
      },
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/RoBERTa_files/test.csv')\r\n",
        "test.insert(3,'selected_text', test.text)\r\n",
        "test_dataset = TextDataset(text = test.text.values,\r\n",
        "                              selected_text = test.selected_text.values,\r\n",
        "                              sentiment = test.sentiment.values)\r\n",
        "\r\n",
        "test_dataloader = DataLoader(test_dataset,\r\n",
        "                              batch_size = 16,\r\n",
        "                              shuffle = False,\r\n",
        "                              num_workers=1)\r\n",
        "jaccs, pred = evaluate_fn(test_dataloader,model0,device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Average loss on validation data: 0.00\n",
            "  Average jaccard similarity on validation data: 1.00\n",
            "  validation took: 0:02:30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW02UeiISnny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9faf75d3-287b-49bc-acb9-5f109befcab5"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/submission.csv')\r\n",
        "submission.selected_text = pred\r\n",
        "submission.sample(40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>f0ef04109b</td>\n",
              "      <td>about to go to sleep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2564</th>\n",
              "      <td>d528912ba9</td>\n",
              "      <td>: nice to see you on twitter!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>49e9fe8b96</td>\n",
              "      <td>i want to go to singapore but my mother seems...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>8c08631422</td>\n",
              "      <td>On the phone to mum http://tinyurl.com/otdn9u</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1798</th>\n",
              "      <td>198854f289</td>\n",
              "      <td>cannot relaxing because she have to practice ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3376</th>\n",
              "      <td>9e734c1ed3</td>\n",
              "      <td>I`ll put your name on the list</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2386</th>\n",
              "      <td>d276268ebf</td>\n",
              "      <td>Uh-oh...it`s becoming grey again out here. I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>b357b9e7d2</td>\n",
              "      <td>school. 39 days !!! so exited. Amazing premier</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3432</th>\n",
              "      <td>d3b70656cd</td>\n",
              "      <td>_marie:my heart goes out to you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>1e41208494</td>\n",
              "      <td>alas no, it`s just a normal night monday is q...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2162</th>\n",
              "      <td>27d0c01bc2</td>\n",
              "      <td>I REEEALLY wish I could be there haven`t been...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2538</th>\n",
              "      <td>6dc1aa9db8</td>\n",
              "      <td>hey Gerardo! (late response) that day i was t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2534</th>\n",
              "      <td>4c2cb5387a</td>\n",
              "      <td>god i cant even catch public transport. swine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2506</th>\n",
              "      <td>d8c30b8a27</td>\n",
              "      <td>_carter The video is set to private</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>c002c2eddd</td>\n",
              "      <td>Watching One Fine Day while eating my cereal....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2459</th>\n",
              "      <td>c2645bfd2f</td>\n",
              "      <td>ano pa bang aasahan ko sa iyo? you never fail...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1646</th>\n",
              "      <td>b6262b919a</td>\n",
              "      <td>Wow, my bed is SO comfy &amp; my nap has been muc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1723</th>\n",
              "      <td>3ec9a1578c</td>\n",
              "      <td>Spilled chocolate milk in my car</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1790</th>\n",
              "      <td>10c841e57b</td>\n",
              "      <td>anyone got an FFE account? if so.. add me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>25510fbf89</td>\n",
              "      <td>I have the Job this is a nice day it can not ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>97e53162ff</td>\n",
              "      <td>dear oh dear.....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>772ba56ce6</td>\n",
              "      <td>Morning all! Have a GREAT DAY! Off to school ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3128</th>\n",
              "      <td>c29116d111</td>\n",
              "      <td>happy Star Wars day</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>ec00ec25c7</td>\n",
              "      <td>has dislocated her knee *bad/painful times* g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>47f11efc72</td>\n",
              "      <td>this creepy guy when I was walking the dog I`...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1279</th>\n",
              "      <td>7013b54ea6</td>\n",
              "      <td>_ Heading off to Poole around 4ish, has some ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3200</th>\n",
              "      <td>88040d7737</td>\n",
              "      <td>thanks to follow. have a nice rest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2486</th>\n",
              "      <td>77de10258b</td>\n",
              "      <td>Remmber time crisis ? Try it on the #iPhone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2184</th>\n",
              "      <td>3775f77081</td>\n",
              "      <td>having my dinner. eating bangus. it`s a fish.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1674</th>\n",
              "      <td>e6c2edcb0b</td>\n",
              "      <td>back from melly`s party... i had fun... i`m s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>a11da2d3bf</td>\n",
              "      <td>Playin City of Villains, wishin my buddies we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2446</th>\n",
              "      <td>b1caa80c14</td>\n",
              "      <td>Just finished watching Star Trek in IMAX. . ....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td>a9bc0bc35d</td>\n",
              "      <td>i agree with the whole Hollie thing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2995</th>\n",
              "      <td>1872e32e07</td>\n",
              "      <td>I feel like I`m on alott of drugs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>bc3032341b</td>\n",
              "      <td>i`m sad...i`ll miss you grandma angie.. you w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>598603422e</td>\n",
              "      <td>Shoot. Only 12 miles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>e0f9a074cf</td>\n",
              "      <td>Now I like #startrek. Personal feels that #St...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>4d21d7611c</td>\n",
              "      <td>He`s an amazing jockey! Saw that Clydesdale c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2144</th>\n",
              "      <td>8e3b60deaa</td>\n",
              "      <td>No problem. At least look on the floor. We wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>8429087609</td>\n",
              "      <td>Wango tango!!! Good night all</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID                                      selected_text\n",
              "23    f0ef04109b                               about to go to sleep\n",
              "2564  d528912ba9                      : nice to see you on twitter!\n",
              "371   49e9fe8b96   i want to go to singapore but my mother seems...\n",
              "695   8c08631422      On the phone to mum http://tinyurl.com/otdn9u\n",
              "1798  198854f289   cannot relaxing because she have to practice ...\n",
              "3376  9e734c1ed3                     I`ll put your name on the list\n",
              "2386  d276268ebf   Uh-oh...it`s becoming grey again out here. I ...\n",
              "310   b357b9e7d2     school. 39 days !!! so exited. Amazing premier\n",
              "3432  d3b70656cd                    _marie:my heart goes out to you\n",
              "857   1e41208494   alas no, it`s just a normal night monday is q...\n",
              "2162  27d0c01bc2   I REEEALLY wish I could be there haven`t been...\n",
              "2538  6dc1aa9db8   hey Gerardo! (late response) that day i was t...\n",
              "2534  4c2cb5387a   god i cant even catch public transport. swine...\n",
              "2506  d8c30b8a27                _carter The video is set to private\n",
              "1466  c002c2eddd   Watching One Fine Day while eating my cereal....\n",
              "2459  c2645bfd2f   ano pa bang aasahan ko sa iyo? you never fail...\n",
              "1646  b6262b919a   Wow, my bed is SO comfy & my nap has been muc...\n",
              "1723  3ec9a1578c                   Spilled chocolate milk in my car\n",
              "1790  10c841e57b          anyone got an FFE account? if so.. add me\n",
              "136   25510fbf89   I have the Job this is a nice day it can not ...\n",
              "401   97e53162ff                                  dear oh dear.....\n",
              "286   772ba56ce6   Morning all! Have a GREAT DAY! Off to school ...\n",
              "3128  c29116d111                                happy Star Wars day\n",
              "1244  ec00ec25c7   has dislocated her knee *bad/painful times* g...\n",
              "418   47f11efc72   this creepy guy when I was walking the dog I`...\n",
              "1279  7013b54ea6   _ Heading off to Poole around 4ish, has some ...\n",
              "3200  88040d7737                 thanks to follow. have a nice rest\n",
              "2486  77de10258b        Remmber time crisis ? Try it on the #iPhone\n",
              "2184  3775f77081      having my dinner. eating bangus. it`s a fish.\n",
              "1674  e6c2edcb0b   back from melly`s party... i had fun... i`m s...\n",
              "120   a11da2d3bf   Playin City of Villains, wishin my buddies we...\n",
              "2446  b1caa80c14   Just finished watching Star Trek in IMAX. . ....\n",
              "1109  a9bc0bc35d                i agree with the whole Hollie thing\n",
              "2995  1872e32e07                  I feel like I`m on alott of drugs\n",
              "1549  bc3032341b   i`m sad...i`ll miss you grandma angie.. you w...\n",
              "744   598603422e                               Shoot. Only 12 miles\n",
              "1452  e0f9a074cf   Now I like #startrek. Personal feels that #St...\n",
              "2221  4d21d7611c   He`s an amazing jockey! Saw that Clydesdale c...\n",
              "2144  8e3b60deaa   No problem. At least look on the floor. We wo...\n",
              "815   8429087609                      Wango tango!!! Good night all"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdCkFxbiTDld",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "782a57ad-4dca-4237-d940-0ec83a3da7af"
      },
      "source": [
        "from google.colab import files\r\n",
        "submission.to_csv('submission.csv') \r\n",
        "files.download('submission.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2f91992e-1697-4b92-873a-c21f91948b97\", \"submission.csv\", 300697)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fujuq7wPPCXd"
      },
      "source": [
        "### refrence for code\r\n",
        "[Roberta one fold](https://www.kaggle.com/abhishek/multiprocessing-roberta-1-fold-per-core)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKpAEOzmFNy4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}