{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence Classification with Transformers",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrakhshanda/Text-Mining/blob/main/Sequence_Classification_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipMamgbw6bjL"
      },
      "source": [
        "# Sequence Classification with Transformers\n",
        "\n",
        "This colab notebook will guide you through using the Transformers library to obtain state-of-the-art results on the sequence classification task. It is attached to [the following tutorial](https://medium.com/@lysandrejik/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a).\n",
        "\n",
        "We will be using two different models as a means of comparison: Google's BERT and Facebook's RoBERTa. Both have the same architecture but have had different pre-training approached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KAjNG_J7K-w"
      },
      "source": [
        "## Installing required dependencies\n",
        "In order to import the TensorFlow modules, we must make sure that TF2 is installed in the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMvKYmV48aHi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5fde8b3a-c567-4847-835f-0edbf6a724dc"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH6P00pLYbXH"
      },
      "source": [
        "#!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIzE-Nj5yvf"
      },
      "source": [
        "## Initializing the pre-trained models\n",
        "\n",
        "Let's initialize the models with pre-trained weights. The list of pre-trained weights is available in [the official documentation](https://huggingface.co/transformers/pretrained_models.html). Downloading the weights may take a bit of time, but it only needs to be done once!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aihC2QV6IXk"
      },
      "source": [
        "from transformers import (TFBertForSequenceClassification, \n",
        "                          BertTokenizer,\n",
        "                          TFRobertaForSequenceClassification, \n",
        "                          RobertaTokenizer)\n",
        "\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "roberta_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zXIt_LK9TWt"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "BERT and RoBERTa are both Transformer models that have the same architecture. As such, they accept only a certain kind of inputs: vectors of integers, each value representing a token. Each string of text must first be converted to a list of indices to be fed to the model. The tokenizer takes care of that for us.\n",
        "\n",
        "BERT and RoBERTa may have the same architecture, but they differ in tokenization. BERT uses a sub-word tokenization, whereas RoBERTa uses the same tokenization than GPT-2: byte-level byte-pair-encoding. Let's see what this means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZRFEqLc8DV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "df993838-bf69-4a8a-cd9f-209658340eb8"
      },
      "source": [
        "sequence = \"Systolic arrays are cool. This 🐳 is cool too.\"\n",
        "\n",
        "bert_tokenized_sequence = bert_tokenizer.tokenize(sequence)\n",
        "roberta_tokenized_sequence = roberta_tokenizer.tokenize(sequence)\n",
        "\n",
        "print(\"BERT:\", bert_tokenized_sequence)\n",
        "print(\"RoBERTa:\", roberta_tokenized_sequence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT: ['S', '##ys', '##to', '##lic', 'array', '##s', 'are', 'cool', '.', 'This', '[UNK]', 'is', 'cool', 'too', '.']\n",
            "RoBERTa: ['Sy', 'st', 'olic', 'Ġarrays', 'Ġare', 'Ġcool', '.', 'ĠThis', 'ĠðŁ', 'Ĳ', '³', 'Ġis', 'Ġcool', 'Ġtoo', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W19ze-Fd_RqD"
      },
      "source": [
        "**BERT Tokenizer**\n",
        "\n",
        "Here, the BERT tokenizer splits the string into multiple substrings. If the substrings are in its vocabulary, they will stay as is: this is the case for `array`,  `are` and  `cool`. However, if a resulting string is not in its vocabulary, it will be split again until every string is represented by its vocabulary. For example,  `Systolic` is split multiple times until every token is represented in the BERT vocabulary: it is split into four tokens.\n",
        "The BERT tokenizer is lacking when it comes to complex characters spread over multiple bytes, as can be seen with emojis. In the sequence used, an emoji of a whale was added. As the BERT tokenizer cannot interpret this emoji on a byte-level, it replaces it by the unknown token [UNK].\n",
        "\n",
        "**RoBERTa Tokenizer**\n",
        "\n",
        "On the other hand, the RoBERTa tokenizer has a slightly different approach. Here too, the string is split into multiple substrings, which are themselves split into multiple substrings until every substring can be represented by the vocabulary. However, the RoBERTa tokenizer has a **byte-level approach**. This tokenizer can represent every sequence as a combination of bytes, which makes it shine in the case of complex characters spread over multiple bytes, as with the whale emoji. Instead of using the unknown token, this tokenizer can correctly encode the whale emoji as the combination of multiple bytes. This tokenizer therefore does not require an unknown token, as it can handle every byte separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5TVSZtxEl32"
      },
      "source": [
        "## Getting State-of-the-Art results on sequence classification\n",
        "\n",
        "In order to get State-of-the-Art results on this task, we will fine-tune our models on a given dataset. Fine-tuning a model means that we will slightly train it on top of an already trained checkpoint. The learning rate will be very low, as having it to high would result in catastrophic forgetting -> the model would forget what it had learned until now semantically and syntaxically.\n",
        "\n",
        "We will follow the procedure detailed below:\n",
        "\n",
        "    1) Get the dataset from `tensorflow_datasets`\n",
        "\n",
        "    2) Pre-process this dataset so that it can be used by the model\n",
        "\n",
        "    3) Set-up a training loop using Keras' fit API; train the model on the training data\n",
        "\n",
        "    4) Evaluate the model on the testing data and compare to the actual results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ImFqNidSIk9"
      },
      "source": [
        "### Getting the dataset\n",
        "\n",
        "We will be using the Microsoft Research Paraphrase Corpus (MRPC) dataset, which is a sequence classification dataset. We get the train and validation data from the `tensorflow_datasets` package. These values are in the form of `tf.data.Dataset`, which is perfect for our use-case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro-ROZtP-GoB"
      },
      "source": [
        "import tensorflow_datasets\n",
        "data = tensorflow_datasets.load(\"glue/mrpc\")\n",
        "\n",
        "train_dataset = data[\"train\"]\n",
        "validation_dataset = data[\"validation\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr07j8B2TiEF"
      },
      "source": [
        "Let's output the value of the first item of the dataset to get an idea of the data we're dealing with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDHPqzQuQm9G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a08f88f4-9445-46e0-c79f-ba86d213ecd4"
      },
      "source": [
        "example = list(train_dataset.__iter__())[0]\n",
        "print('',\n",
        "    'idx:      ', example['idx'],       '\\n',\n",
        "    'label:    ', example['label'],     '\\n',\n",
        "    'sentence1:', example['sentence1'], '\\n',\n",
        "    'sentence2:', example['sentence2'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " idx:       tf.Tensor(201, shape=(), dtype=int32) \n",
            " label:     tf.Tensor(1, shape=(), dtype=int64) \n",
            " sentence1: tf.Tensor(b'Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company .', shape=(), dtype=string) \n",
            " sentence2: tf.Tensor(b'Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cfNc0YyT6fj"
      },
      "source": [
        "The values are in the form of dictionaries that are done as follow:\n",
        "\n",
        "```py\n",
        "example = {\n",
        "    idx: number,\n",
        "    label: number,\n",
        "    sentence1: string,\n",
        "    sentence2: string\n",
        "}\n",
        "```\n",
        "The three fields that are of interest to us are the `label`, the `sentence1` and the `sentence2`. The label is equal to `1` when the two sentences are paraphrases of each other, and is equal to `0` otherwise.\n",
        "\n",
        "We cannot pass this directly to the models as they cannot interpret the meaning of strings (list of characters), so we will see how to convert that example to features that our models can understand. Firstly, we must obtain the token ids that will be fed to the model from the two sequences. The two models (BERT, RoBERTa) have different encoding mechanisms concerning pair sequence encoding, using `sep` and `cls` tokens to specify the end of a sequence or the end of the two sequences. With `A` as the first sequence and `B` as the second, BERT encodes the sequence pair as follows:\n",
        "\n",
        "`[CLS] A [SEP] B [SEP]`\n",
        "\n",
        "while RoBERTa encodes the sequence pair differently:\n",
        "\n",
        "`[CLS] A [SEP][SEP] B [SEP]`\n",
        "\n",
        "Thankfully, our encode method can handle that on its own. Here are the two sentences we are currently dealing with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUW95hbHVuGn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "951d14f8-7daa-4d61-d296-4a1eeed98b90"
      },
      "source": [
        "seq0 = example['sentence1'].numpy().decode('utf-8')  # Obtain bytes from tensor and convert it to a string\n",
        "seq1 = example['sentence2'].numpy().decode('utf-8')  # Obtain bytes from tensor and convert it to a string\n",
        "\n",
        "print(\"First sequence:\", seq0)\n",
        "print(\"Second sequence:\", seq1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First sequence: Tibco has used the Rendezvous name since 1994 for several of its technology products , according to the Palo Alto , California company .\n",
            "Second sequence: Tibco has used the Rendezvous name since 1994 for several of its technology products , it said .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMip2J9Iuqoq"
      },
      "source": [
        "## Encoding sequences\n",
        "\n",
        "In order to encode the sequence to be understandable by the model, two different methods can come in handy.\n",
        "\n",
        "### encode()\n",
        "\n",
        "`encode` is a high-level method that returns the encoded sequence with the special tokens and truncated to a maximum length if need be. Here we identify the special CLS and SEP tokens of RoBERTa and BERT, and explicit them in the encoded sequence as to understand the difference in tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR9yka31ul3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "c7403ec3-e3e6-4700-c31f-57edebe219de"
      },
      "source": [
        "encoded_bert_sequence = bert_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)\n",
        "encoded_roberta_sequence = roberta_tokenizer.encode(seq0, seq1, add_special_tokens=True, max_length=128)\n",
        "\n",
        "print(\"BERT tokenizer separator, cls token id:   \", bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id)\n",
        "print(\"RoBERTa tokenizer separator, cls token id:\", roberta_tokenizer.sep_token_id, roberta_tokenizer.cls_token_id)\n",
        "\n",
        "bert_special_tokens = [bert_tokenizer.sep_token_id, bert_tokenizer.cls_token_id]\n",
        "roberta_special_tokens = [roberta_tokenizer.sep_token_id, roberta_tokenizer.cls_token_id]\n",
        "\n",
        "def print_in_red(string):\n",
        "    print(\"\\033[91m\" + str(string) + \"\\033[0m\", end=' ')\n",
        "\n",
        "print(\"\\nBERT tokenized sequence\")\n",
        "output = [print_in_red(tok) if tok in bert_special_tokens else print(tok, end=' ') for tok in encoded_bert_sequence]\n",
        "\n",
        "print(\"\\n\\nRoBERTa tokenized sequence\")\n",
        "output = [print_in_red(tok) if tok in roberta_special_tokens else print(tok, end=' ') for tok in encoded_roberta_sequence]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT tokenizer separator, cls token id:    102 101\n",
            "RoBERTa tokenizer separator, cls token id: 2 0\n",
            "\n",
            "BERT tokenized sequence\n",
            "\u001b[91m101\u001b[0m 157 13292 2528 1144 1215 1103 16513 15125 11944 1271 1290 1898 1111 1317 1104 1157 2815 2982 117 2452 1106 1103 19585 2858 17762 117 1756 1419 119 \u001b[91m102\u001b[0m 157 13292 2528 1144 1215 1103 16513 15125 11944 1271 1290 1898 1111 1317 1104 1157 2815 2982 117 1122 1163 119 \u001b[91m102\u001b[0m \n",
            "\n",
            "RoBERTa tokenized sequence\n",
            "\u001b[91m0\u001b[0m 565 1452 876 34 341 5 29110 42057 766 187 8148 13 484 9 63 806 785 2156 309 7 5 21065 18402 2156 886 138 479 \u001b[91m2\u001b[0m \u001b[91m2\u001b[0m 565 1452 876 34 341 5 29110 42057 766 187 8148 13 484 9 63 806 785 2156 24 26 479 \u001b[91m2\u001b[0m "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_apacxCvZZ7"
      },
      "source": [
        "### encode_plus()\n",
        "\n",
        "`encode_plus` is similar to `encode` but it returns additional information: the token type ids as well as several other features that we don't need to manage right now.\n",
        "\n",
        "The token type ids are used by some models in the case of sequence classification. It is a mask indicating to the model which sequence a token is from. \n",
        "\n",
        "For example, let's say we have two sequences A and B with tokens `[a0, a1, a2, a3]` and `[b0, b1, b2, b3, b4]` respectively.\n",
        "\n",
        "The BERT tokenizer would create a single sequence from those two lists of tokens that would look like the following:\n",
        "\n",
        "<pre>\n",
        "[tokens]         `[CLS] a0 a1 a2 a3 [SEP] b0 b1 b2 b3 b4 [SEP]`. \n",
        "[token type ids] `  0    0  0  0  0   0    1  1  1  1  1   1`\n",
        "</pre>\n",
        "\n",
        "Thanks to the token type ids, the model is aware of which token belongs to which sequence\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73XQoKaBb1GG"
      },
      "source": [
        "We won't need to use encode_plus in this experiment as directly in the `Transformers` library exists a method to directly convert a dataset to features, and is agnostic to both the GLUE task and the specified tokenizer. This method makes use of `encode_plus` under the hood and is called `glue_convert_examples_to_features`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTi1PqEoX-Sq"
      },
      "source": [
        "from transformers import glue_convert_examples_to_features\n",
        "\n",
        "bert_train_dataset = glue_convert_examples_to_features(train_dataset, bert_tokenizer, 128, 'mrpc')\n",
        "bert_train_dataset = bert_train_dataset.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "bert_validation_dataset = glue_convert_examples_to_features(validation_dataset, bert_tokenizer, 128, 'mrpc')\n",
        "bert_validation_dataset = bert_validation_dataset.batch(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AERtOk6LfUT"
      },
      "source": [
        "The two BERT datasets are now ready to be used: the training dataset is shuffled and batch, while the validation dataset is only batched.\n",
        "\n",
        "RoBERTa requires a bit more of work as it does not use the `token_type_ids`, which we need to remove. We use the `tf.data.Dataset.map()` method for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbdrVn7BuFSJ"
      },
      "source": [
        "def token_type_ids_removal(example, label):\n",
        "    del example[\"token_type_ids\"]\n",
        "    return example, label\n",
        "\n",
        "roberta_train_dataset = glue_convert_examples_to_features(train_dataset, roberta_tokenizer, 128, 'mrpc')\n",
        "roberta_train_dataset = roberta_train_dataset.map(token_type_ids_removal)\n",
        "roberta_train_dataset = roberta_train_dataset.shuffle(100).batch(32).repeat(2)\n",
        "\n",
        "roberta_validation_dataset = glue_convert_examples_to_features(validation_dataset, roberta_tokenizer, 128, 'mrpc')\n",
        "roberta_validation_dataset = roberta_validation_dataset.map(token_type_ids_removal)\n",
        "roberta_validation_dataset = roberta_validation_dataset.batch(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOFkCP7seqHw"
      },
      "source": [
        "### Defining the hyper-parameters\n",
        "\n",
        "Before fine-tuning the model, we must define a few hyperparameters that will be used during the training such as the optimizer, the loss and the evaluation metric.\n",
        "\n",
        "As an optimizer we'll be using Adam, which was the optimizer used during those models' pre-training. As a loss we'll be using the sparse categorical cross-entropy, and the sparse categorical accuracy as the evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQup8yJ2eDHq"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "roberta_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2reI1c2HqfIt"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "The beauty of tensorflow/keras lies here: using keras' fit method to fine-tune the model with a single line of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soy6ZW-mqp0r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "4516e1c5-6fda-49a6-8184-1005c442cfbe"
      },
      "source": [
        "print(\"Fine-tuning BERT on MRPC\")\n",
        "bert_history = bert_model.fit(bert_train_dataset, epochs=3, validation_data=bert_validation_dataset)\n",
        "\n",
        "print(\"\\nFine-tuning RoBERTa on MRPC\")\n",
        "roberta_history = roberta_model.fit(roberta_train_dataset, epochs=3, validation_data=roberta_validation_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fine-tuning BERT on MRPC\n",
            "Epoch 1/3\n",
            "230/230 [==============================] - 376s 2s/step - loss: 0.4896 - accuracy: 0.7601 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "230/230 [==============================] - 346s 2s/step - loss: 0.1541 - accuracy: 0.9434 - val_loss: 0.5791 - val_accuracy: 0.8456\n",
            "Epoch 3/3\n",
            "230/230 [==============================] - 347s 2s/step - loss: 0.0567 - accuracy: 0.9804 - val_loss: 0.5465 - val_accuracy: 0.8505\n",
            "\n",
            "Fine-tuning RoBERTa on MRPC\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "230/230 [==============================] - 377s 2s/step - loss: 0.6363 - accuracy: 0.6728 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/3\n",
            "230/230 [==============================] - 349s 2s/step - loss: 0.6362 - accuracy: 0.6745 - val_loss: 0.6281 - val_accuracy: 0.6838\n",
            "Epoch 3/3\n",
            "230/230 [==============================] - 348s 2s/step - loss: 0.6351 - accuracy: 0.6745 - val_loss: 0.6280 - val_accuracy: 0.6838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YljeshET-_7U"
      },
      "source": [
        "### Keras's simplicity doesn't end here\n",
        "\n",
        "Evaluating a model is as simple as it is to train it - using the evaluate method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scT82c9arCRv"
      },
      "source": [
        "print(\"Evaluating the BERT model\")\n",
        "bert_model.evaluate(bert_validation_dataset)\n",
        "\n",
        "print(\"Evaluating the RoBERTa model\")\n",
        "roberta_model.evaluate(roberta_validation_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj55Uq77tQ6J"
      },
      "source": [
        "## Results\n",
        "\n",
        "The results we obtain for BERT are similar to the paper's original results, which were computed using the official GLUE evaluation server. The accuracy obtained for RoBERTa is slightly less than in the paper, which is probably due to the initialisation done: in the paper, the fine-tuning on the MRPC task was done from the MNLI checkpoint rather than from the base checkpoint."
      ]
    }
  ]
}