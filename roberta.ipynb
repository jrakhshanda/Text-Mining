{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3pl3CsbbQ3pH+UAFmQVl6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrakhshanda/Text-Mining/blob/main/roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekcMUspsNFY2",
        "outputId": "28960e7a-41e0-4158-d0b0-7b6490c4d8df"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVGoYWYBP9gs"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install tokenizers\r\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\r\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\r\n",
        "#!export XLA_USE_BF16=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9gYGcLNTTz"
      },
      "source": [
        "import os\r\n",
        "import string\r\n",
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn import model_selection\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from transformers import *\r\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
        "import tokenizers"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWxMIPZy5W5T",
        "outputId": "bc30cb08-8992-4339-fcc2-854c209fada3"
      },
      "source": [
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7q1nawUPMqT"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgd78eqmNbzc"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\r\n",
        "class config:\r\n",
        "    TRAIN_BATCH_SIZE = 32\r\n",
        "    VALID_BATCH_SIZE = 8\r\n",
        "    EPOCHS = 7\r\n",
        "    PATH = '/content/drive/MyDrive/RoBERTa_files'\r\n",
        "    TRAINING_FILE = pd.read_csv('/content/drive/MyDrive/BERT_files/df_train.csv')\r\n",
        "    TEST_FILE =  pd.read_csv('/content/drive/MyDrive/BERT_files/df_train.csv')\r\n",
        "    MAX_LEN = 141\r\n",
        "    TOKENIZER = ByteLevelBPETokenizer(f\"{PATH}/vocab.json\",\r\n",
        "                                      f\"{PATH}/merges.txt\",\r\n",
        "                                      lowercase=True, add_prefix_space=True)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOXQ5K2XA-l"
      },
      "source": [
        "# Processing of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-qngeJ3Xahc"
      },
      "source": [
        "def process_data(text, selected_text, sentiment, tokenizer=config.TOKENIZER, max_len=config.MAX_LEN):\r\n",
        "    # roberta requires the text to have a prefix space at the beginning\r\n",
        "    text = \" \" + \" \".join(str(text).split(\" \"))\r\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split(\" \"))\r\n",
        "\r\n",
        "    # getting initial and final index of selected_text within the text\r\n",
        "    len_selected = len(selected_text) - 1\r\n",
        "    idx1 = idx2 = None\r\n",
        "    for idx, letter in enumerate(selected_text):\r\n",
        "        if (text[idx] == selected_text[1]) and (\" \" + text[idx: idx+len_selected] == selected_text):\r\n",
        "            idx1 = idx\r\n",
        "            idx2 = idx1 + len_selected - 1\r\n",
        "            break\r\n",
        "    \r\n",
        "    # making character targets\r\n",
        "    if idx1!=None and idx2!=None:\r\n",
        "        char_targets = [0] * len(text)\r\n",
        "        for i in range(idx1, idx2+1):\r\n",
        "            char_targets[i] = 1\r\n",
        "    else:\r\n",
        "        char_targets = [1] * len(text)\r\n",
        "\r\n",
        "    # encoding using pretrained tokenizer\r\n",
        "    tok_text = tokenizer.encode(text)\r\n",
        "    ids = tok_text.ids\r\n",
        "    attention_mask = tok_text.attention_mask\r\n",
        "    type_ids = tok_text.type_ids\r\n",
        "\r\n",
        "    # getting indexes of tokens containing character in selected_text\r\n",
        "    target_idx = []\r\n",
        "    for i, (offset1, offset2) in enumerate(tok_text.offsets):\r\n",
        "        if sum(char_targets[offset1: offset2])>0:\r\n",
        "            target_idx.append(i)\r\n",
        "\r\n",
        "    # we just need the indexes of the start and end tokens as we are using \r\n",
        "    # nn. CrossEntropy as loss\r\n",
        "    start_target = target_idx[0]\r\n",
        "    end_target = target_idx[-1]\r\n",
        "\r\n",
        "    # token ids of sentiment as present in our vocab hard coded here\r\n",
        "    sentiment_ids = {\r\n",
        "        'positive':1313,                    # tokenizer.encode('positive').ids\r\n",
        "        'negative':2430,                    # tokenizer.encode('negative').ids\r\n",
        "        'neutral':7974                     # tokenizer.encode('neutral').ids\r\n",
        "    }\r\n",
        "\r\n",
        "    # adding special tokens\r\n",
        "    ids = [0] + [sentiment_ids[sentiment]] + [2] + [2] + ids + [2]\r\n",
        "    attention_mask = [1] * len(ids)\r\n",
        "    type_ids = [0] * len(ids)\r\n",
        "    offsets = [(0, 0)] * 4 + tok_text.offsets\r\n",
        "    start_target += 4\r\n",
        "    end_target += 4\r\n",
        "\r\n",
        "    # padding\r\n",
        "    padding_len = max_len - len(ids)\r\n",
        "    if padding_len>0:\r\n",
        "        ids = ids + [1] * padding_len\r\n",
        "        attention_mask = attention_mask + [0] * padding_len\r\n",
        "        type_ids = type_ids + [0] * padding_len\r\n",
        "        offsets = offsets + [(0, 0)] * padding_len\r\n",
        "\r\n",
        "    return {\r\n",
        "        'ids': torch.tensor(ids,dtype=torch.long),\r\n",
        "        'attention_mask': torch.tensor(attention_mask,dtype=torch.long),\r\n",
        "        'token_type_ids':torch.tensor(type_ids,dtype=torch.long),\r\n",
        "        'targets_start': torch.tensor(start_target,dtype=torch.long),\r\n",
        "        'targets_end':  torch.tensor(end_target,dtype=torch.long),\r\n",
        "        'offsets': torch.tensor(offsets,dtype=torch.long),\r\n",
        "        'padding_len': padding_len,\r\n",
        "        'text': text,\r\n",
        "        'selected_text': selected_text,\r\n",
        "        'sentiment': sentiment\r\n",
        "    }"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW51NizeXrjl"
      },
      "source": [
        "class TextDataset(Dataset):\r\n",
        "    def __init__(self, text, sentiment, selected_text):\r\n",
        "        self.text = text\r\n",
        "        self.sentiment = sentiment\r\n",
        "        self.selected_text = selected_text\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.text)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        # processing data\r\n",
        "        data = process_data(\r\n",
        "            self.text[item], \r\n",
        "            self.selected_text[item], \r\n",
        "            self.sentiment[item]\r\n",
        "        )\r\n",
        "        # returning tensors\r\n",
        "        return data"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN2ApvivZSEo",
        "outputId": "d9dd532a-d542-4e4a-855d-29f4e0af139f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#import pdb\r\n",
        "#pdb.set_trace()\r\n",
        "df = config.TRAINING_FILE.reset_index(drop=True)\r\n",
        "if __name__== \"__main__\":\r\n",
        "  dset = TextDataset(text = df.text.values,\r\n",
        "                      selected_text =df.selected_text.values,sentiment = df.sentiment.values)\r\n",
        "  print(dset[5])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ids': tensor([    0,  7974,     2,     2, 36778, 10242,  3923,   275,   910, 15574,\n",
            "         7900,  6872,     2,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'targets_start': tensor(4), 'targets_end': tensor(11), 'offsets': tensor([[ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0, 10],\n",
            "        [10, 15],\n",
            "        [15, 19],\n",
            "        [19, 24],\n",
            "        [24, 26],\n",
            "        [26, 32],\n",
            "        [32, 38],\n",
            "        [38, 44],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0]]), 'padding_len': 128, 'text': ' shameless plugging best rangers forum earth', 'selected_text': ' shameless plugging best rangers forum earth', 'sentiment': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwNpnriG8GZv"
      },
      "source": [
        "Now weâ€™ll create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6C8eZVl8NYc"
      },
      "source": [
        "dfx = config.TRAINING_FILE\r\n",
        "dfx.reset_index(drop=True)\r\n",
        "# spliting into training and validation set\r\n",
        "train, valid = model_selection.train_test_split(dfx,\r\n",
        "                                                      test_size=0.1,\r\n",
        "                                                      random_state=42,\r\n",
        "                                                      stratify=dfx.sentiment.values)\r\n",
        "\r\n",
        "# using TextDataset function as coded above\r\n",
        "train_dataset = TextDataset(text=train.text.values,\r\n",
        "                            sentiment=train.sentiment.values,\r\n",
        "                            selected_text=train.selected_text.values)\r\n",
        "\r\n",
        "valid_dataset = TextDataset(text=valid.text.values,\r\n",
        "                            sentiment=valid.sentiment.values,\r\n",
        "                            selected_text=valid.selected_text.values)\r\n",
        "\r\n",
        "# making pytorch dataloaders\r\n",
        "train_data_loader = DataLoader(train_dataset,\r\n",
        "                               batch_size=config.TRAIN_BATCH_SIZE)\r\n",
        "\r\n",
        "valid_data_loader = DataLoader(valid_dataset,batch_size=config.VALID_BATCH_SIZE)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcCUgOU8PRA2"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eadEGBNw76"
      },
      "source": [
        "class TextModel(BertPreTrainedModel):\r\n",
        "    def __init__(self,conf):\r\n",
        "        super(TextModel, self).__init__(conf)\r\n",
        "\r\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\",config = conf)\r\n",
        "        self.drop_out = nn.Dropout(0.1)\r\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\r\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\r\n",
        "        # this is to initialize the weights of the matrix that would convert \r\n",
        "        # (batch_size, max_len, 2*768) to (batch_size, max_len, 1) with std=0.02 \r\n",
        "    \r\n",
        "    def forward(self, ids, mask, token_type_ids):\r\n",
        "        _, _, output = self.roberta(\r\n",
        "            ids,\r\n",
        "            attention_mask = attention_mask,\r\n",
        "            token_type_ids=token_type_ids\r\n",
        "        )\r\n",
        "        # out dim = (12, batch_size, max_len, 768)\r\n",
        "        # 12 denotes the 12 hidden layers of roberta\r\n",
        "\r\n",
        "        output = torch.cat((output[-1], output[-2]), dim=-1)\r\n",
        "        # output dim = (batch_size, max_len, 2*768)\r\n",
        "        output = self.drop_out(output)\r\n",
        "        logits = self.l0(output)\r\n",
        "        # logits dim -> (batch_size, max_len, 2)\r\n",
        "\r\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len, 1)\r\n",
        "\r\n",
        "        start_logits = start_logits.squeeze(-1)\r\n",
        "        end_logits = end_logits.squeeze(-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len)\r\n",
        "\r\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAZfDEzyjy-V",
        "outputId": "c4d93d3c-9c26-4fda-ceea-bcc2fbf2bf5b"
      },
      "source": [
        "conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "model = TextModel(conf)\r\n",
        "model.to(device)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextModel(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (drop_out): Dropout(p=0.1, inplace=False)\n",
              "  (l0): Linear(in_features=1536, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lU4SNuJLsiC",
        "outputId": "72b8f0b9-d7ae-4b16-8c51-050d0fd31e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\r\n",
        "params = list(model.named_parameters())\r\n",
        "print('The RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\r\n",
        "print('==== Embedding Layer ====\\n')\r\n",
        "for p in params[0:5]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "print('\\n==== First Transformer ====\\n')\r\n",
        "for p in params[5:21]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "print('\\n==== Output Layer ====\\n')\r\n",
        "for p in params[-4:]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The RoBERTa model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "roberta.embeddings.word_embeddings.weight               (50265, 768)\n",
            "roberta.embeddings.position_embeddings.weight             (514, 768)\n",
            "roberta.embeddings.token_type_embeddings.weight             (1, 768)\n",
            "roberta.embeddings.LayerNorm.weight                           (768,)\n",
            "roberta.embeddings.LayerNorm.bias                             (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "roberta.encoder.layer.0.attention.self.query.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.query.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.self.key.weight         (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.key.bias               (768,)\n",
            "roberta.encoder.layer.0.attention.self.value.weight       (768, 768)\n",
            "roberta.encoder.layer.0.attention.self.value.bias             (768,)\n",
            "roberta.encoder.layer.0.attention.output.dense.weight     (768, 768)\n",
            "roberta.encoder.layer.0.attention.output.dense.bias           (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
            "roberta.encoder.layer.0.intermediate.dense.weight        (3072, 768)\n",
            "roberta.encoder.layer.0.intermediate.dense.bias              (3072,)\n",
            "roberta.encoder.layer.0.output.dense.weight              (768, 3072)\n",
            "roberta.encoder.layer.0.output.dense.bias                     (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight               (768,)\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias                 (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "roberta.pooler.dense.weight                               (768, 768)\n",
            "roberta.pooler.dense.bias                                     (768,)\n",
            "l0.weight                                                  (2, 1536)\n",
            "l0.bias                                                         (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZjMaPSEYTvi"
      },
      "source": [
        "# loss function. Play around with it and see what works best\r\n",
        "def loss_fn(output_start, output_end, targets_start, targets_end,device):\r\n",
        "  loss = nn.CrossEntropyLoss().to(device)\r\n",
        "  l1 = loss(output_start,targets_start)\r\n",
        "  l2 = loss(output_end,targets_end)\r\n",
        "  return l1 + l2"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI6ea68oYSPq"
      },
      "source": [
        "# jaccard function as mentioned in evaluation section of the contest\r\n",
        "def jaccard_metric(str1, str2): \r\n",
        "    a = set(str1.lower().split()) \r\n",
        "    b = set(str2.lower().split())\r\n",
        "    c = a.intersection(b)\r\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fABYRm3GBTuC"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpBxAHPlVzDB"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\r\n",
        "  total_loss = 0\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for bi, batch in enumerate(data_loader):\r\n",
        "    # getting data\r\n",
        "    ids = batch['ids'].to(device, dtype=torch.long)\r\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "    attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "    targets_start = batch['targets_start'].to(device, dtype=torch.float)\r\n",
        "    targets_end = batch['targets_end'].to(device, dtype=torch.float)\r\n",
        "\r\n",
        "    # zeroing gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # getting outputs\r\n",
        "    output_start, output_end = model(ids,\r\n",
        "                                     attention_mask = attention_mask,\r\n",
        "                                     token_type_ids=token_type_ids)\r\n",
        "    # calulating loss\r\n",
        "    loss = loss_fn(output_start, output_end, targets_start, targets_end)\r\n",
        "    total_loss += loss.item()\r\n",
        "    # calculating gradients\r\n",
        "    loss.backward()\r\n",
        "    # updating model parameters\r\n",
        "    optimizer.step()\r\n",
        "    # stepping learning rate scheduler\r\n",
        "    scheduler.step()\r\n",
        "  \r\n",
        "  avg_train_loss = total_loss / len(data_loader)\r\n",
        "  print(\"\")\r\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "  print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "  return avg_train_loss"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o3V2nFgYHy9"
      },
      "source": [
        "def eval_fn(data_loader, model, device, tokenizer=config.TOKENIZER):\r\n",
        "  total_loss = 0\r\n",
        "  model.eval()\r\n",
        "  # below array will store the respective data\r\n",
        "  all_ids = []\r\n",
        "  start_idx = []\r\n",
        "  end_idx = []\r\n",
        "  selected_text = []\r\n",
        "  padding_len = []\r\n",
        "\r\n",
        "  for bi, batch in enumerate(data_loader):\r\n",
        "    # getting data\r\n",
        "    ids = batch['ids'].to(device, dtype=torch.long)\r\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "    attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "    targets_start = batch['targets_start'].to(device, dtype=torch.float)\r\n",
        "    targets_end = batch['targets_end'].to(device, dtype=torch.float)\r\n",
        "    selected_text = batch['selected_text']\r\n",
        "    pad_len = batch['padding_len']\r\n",
        "\r\n",
        "    # getting output\r\n",
        "    output_start, output_end = model(ids,\r\n",
        "                                     attention_mask = attention_mask,\r\n",
        "                                     token_type_ids=token_type_ids)\r\n",
        "    \r\n",
        "    \r\n",
        "    loss = loss_fn(output_start, output_end, targets_start, targets_end)\r\n",
        "    total_loss += loss.item()\r\n",
        "\r\n",
        "    # adding to array to use latter\r\n",
        "    # also removing stuff from gpu\r\n",
        "    all_ids.append(ids.cpu().detach().numpy())\r\n",
        "    start_idx.append(torch.sigmoid(output_start).cpu().detach().numpy())\r\n",
        "    end_idx.append(torch.sigmoid(output_end).cpu().detach().numpy())\r\n",
        "    selected_text.extend(selected_text)\r\n",
        "    padding_len.extend(pad_len)\r\n",
        "\r\n",
        "    # fixing dimensions\r\n",
        "    start_idx = np.vstack(start_idx)\r\n",
        "    end_idx = np.vstack(end_idx)\r\n",
        "    all_ids = np.vstack(all_ids)\r\n",
        "\r\n",
        "    # to store jaccard score to print mean of it latter\r\n",
        "    jaccards = []\r\n",
        "\r\n",
        "    # getting predicted text and calculating jaccard\r\n",
        "    for i in range(0, len(start_idx)):\r\n",
        "        start_logits = start_idx[i][4: -padding_len[i]-1]\r\n",
        "        end_logits = end_idx[i][4: -padding_len[i]-1]\r\n",
        "        this_id = all_ids[i][4: -padding_len[i]-1]\r\n",
        "\r\n",
        "        idx1 = idx2 = None\r\n",
        "        max_sum = 0\r\n",
        "        for ii, s in enumerate(start_logits):\r\n",
        "            for jj, e in enumerate(end_logits):\r\n",
        "                if  s+e > max_sum:\r\n",
        "                    max_sum = s+e\r\n",
        "                    idx1 = ii\r\n",
        "                    idx2 = jj\r\n",
        "\r\n",
        "        this_id = this_id[idx1: idx2+1]\r\n",
        "        predicted_text = tokenizer.decode(this_id, skip_special_tokens=True)\r\n",
        "        predicted_text = predicted_text.strip()\r\n",
        "        sel_text = selected_text[i].strip()\r\n",
        "\r\n",
        "        jaccards.append(jaccard_metric(predicted_text, sel_text))\r\n",
        "\r\n",
        "  avg_valid_loss = np.mean(total_loss)      \r\n",
        "  print(\"  Average validation loss: {0:.2f}\".format(avg_valid_loss))\r\n",
        "  print(\"  Average jaccard similarity: {0:.2f}\".format(np.mean(jaccards)))\r\n",
        "  print(\"  validation took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "  return np.mean(jaccards), avg_valid_loss"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91QQaFShYT7v"
      },
      "source": [
        "def run():\r\n",
        "    # reading train.csv\r\n",
        "    dfx = config.TRAINING_FILE\r\n",
        "    dfx.reset_index(drop=True)\r\n",
        "    # spliting into training and validation set\r\n",
        "    df_train, df_valid = model_selection.train_test_split(\r\n",
        "        dfx,\r\n",
        "        test_size=0.1,\r\n",
        "        random_state=42,\r\n",
        "        stratify=dfx.sentiment.values)\r\n",
        "\r\n",
        "    # using TextDataset function as coded above\r\n",
        "    train_dataset = TextDataset(\r\n",
        "        text=df_train.text.values,\r\n",
        "        sentiment=df_train.sentiment.values,\r\n",
        "        selected_text=df_train.selected_text.values\r\n",
        "    )\r\n",
        "\r\n",
        "    valid_dataset = TextDataset(\r\n",
        "        text=df_valid.text.values,\r\n",
        "        sentiment=df_valid.sentiment.values,\r\n",
        "        selected_text=df_valid.selected_text.values\r\n",
        "    )\r\n",
        "\r\n",
        "    # making pytorch dataloaders\r\n",
        "    train_data_loader = DataLoader(\r\n",
        "        train_dataset,\r\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\r\n",
        "        num_workers=4\r\n",
        "    )\r\n",
        "\r\n",
        "    valid_data_loader = DataLoader(\r\n",
        "        valid_dataset,\r\n",
        "        batch_size=config.VALID_BATCH_SIZE,\r\n",
        "        num_workers=1\r\n",
        "    )\r\n",
        "\r\n",
        "    # making a instance of the model and putting it into gpu\r\n",
        "    conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "    conf.output_hidden_states = True\r\n",
        "    model = TextModel(conf)\r\n",
        "    model.to(device)\r\n",
        "    \r\n",
        "    # explicitly going through model parameters and removing weight decay\r\n",
        "    # from a few layers \r\n",
        "    param_optimizer = list(model.named_parameters())\r\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\r\n",
        "    optimizer_parameters = [\r\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\r\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\r\n",
        "    ]\r\n",
        "\r\n",
        "    # Coding out the optimizer and scheduler\r\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\r\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "        optimizer,\r\n",
        "        num_warmup_steps=0,\r\n",
        "        num_training_steps=num_train_steps\r\n",
        "    )\r\n",
        "\r\n",
        "    model = nn.DataParallel(model)\r\n",
        "\r\n",
        "    # saving model when we have best jaccard\r\n",
        "    train_loss = []\r\n",
        "    valid_loss = []\r\n",
        "    best_jaccard = 0\r\n",
        "    for epoch in range(config.EPOCHS):\r\n",
        "      # ========================================\r\n",
        "      #               Training\r\n",
        "      # ========================================\r\n",
        "    \r\n",
        "      # Perform one full pass over the training set.\r\n",
        "      print(\"\")\r\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch+1, config.EPOCHS))\r\n",
        "      print('Training...')\r\n",
        "      t0 = time.time()\r\n",
        "      avg_train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\r\n",
        "      train_loss.append(avg_train_loss)\r\n",
        "\r\n",
        "      # ========================================\r\n",
        "      #               Validation\r\n",
        "      # ========================================\r\n",
        "      print(\"\")\r\n",
        "      print(\"Running Validation...\")\r\n",
        "      t0 = time.time()\r\n",
        "        \r\n",
        "      jaccard, avg_valid_loss = eval_fn(valid_data_loader, model, device)\r\n",
        "      valid_loss.append(avg_valid_loss)\r\n",
        "      if epoc == 5:\r\n",
        "        torch.save(model.state_dict(), config.PATH+'/roberta-model1.pth')\r\n",
        "      \r\n",
        "      if epoc == 7:\r\n",
        "        torch.save(model.state_dict(), config.PATH+'/roberta-model2.pth')\r\n",
        "\r\n",
        "      if jaccard > best_jaccard:\r\n",
        "        torch.save(model.state_dict(), config.PATH+'/jaccs.pth')\r\n",
        "        best_jaccard = jaccard\r\n",
        " \r\n",
        "    return train_loss, valid_loss"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "W4eh8U6WYUNM",
        "outputId": "9a92d9ed-5d58-4551-b2a0-8630be1b199a"
      },
      "source": [
        "train_loss, valid_loss = run()"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 7 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-163-aeacb8b8eba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-162-ab9259022eef>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m       \u001b[0mavg_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-160-eea23f967384>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     16\u001b[0m     output_start, output_end = model(ids,\n\u001b[1;32m     17\u001b[0m                                      \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                      token_type_ids=token_type_ids)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# calulating loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'attention_mask'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "1bh50F2_QtZe",
        "outputId": "6567710d-4b7b-40e7-e273-3de020ab0e5e"
      },
      "source": [
        "conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "model = TextModel(conf)\r\n",
        "model = nn.DataParallel(model)\r\n",
        "#model.load_state_dict(torch.load(f\"{config.PATH}/pytorch_model.bin/pytorch_model.bin\"))\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3de44de566cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{config.PATH}/pytorch_model.bin/pytorch_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DataParallel:\n\tMissing key(s) in state_dict: \"module.roberta.embeddings.position_ids\", \"module.roberta.embeddings.word_embeddings.weight\", \"module.roberta.embeddings.position_embeddings.weight\", \"module.roberta.embeddings.token_type_embeddings.weight\", \"module.roberta.embeddings.LayerNorm.weight\", \"module.roberta.embeddings.LayerNorm.bias\", \"module.roberta.encoder.layer.0.attention.self.query.weight\", \"module.roberta.encoder.layer.0.attention.self.query.bias\", \"module.roberta.encoder.layer.0.attention.self.key.weight\", \"module.roberta.encoder.layer.0.attention.self.key.bias\", \"module.roberta.encoder.layer.0.attention.self.value.weight\", \"module.roberta.encoder.layer.0.attention.self.value.bias\", \"module.roberta.encoder.layer.0.attention.output.dense.weight\", \"module.roberta.encoder.layer.0.attention.output.dense.bias\", \"module.roberta.encoder.layer.0.attention.output.LayerNorm.weight\", \"module.roberta.encoder.layer.0.attention.output.LayerNorm.bias\", \"module.roberta.encoder.layer.0.intermediate.dense.weight\", \"module.roberta.encoder.layer.0.intermediate.dense.bias\", \"module.roberta.encoder.layer.0.output.dense.weight\", \"module.roberta.encoder.layer.0.output.dense.bias\", \"module.roberta.encoder.layer.0.output.LayerNorm.weight\", \"module.roberta.encoder.layer.0.output.LayerNorm.bias\", \"module.roberta.encoder.layer.1.attention.self.query.weight\", \"module.roberta.encoder.layer.1.attention.self.query.bias\", \"module.roberta.encoder.layer.1.attention.self.key.weight\", \"module.roberta.encoder.la...\n\tUnexpected key(s) in state_dict: \"roberta.embeddings.word_embeddings.weight\", \"roberta.embeddings.position_embeddings.weight\", \"roberta.embeddings.token_type_embeddings.weight\", \"roberta.embeddings.LayerNorm.weight\", \"roberta.embeddings.LayerNorm.bias\", \"roberta.encoder.layer.0.attention.self.query.weight\", \"roberta.encoder.layer.0.attention.self.query.bias\", \"roberta.encoder.layer.0.attention.self.key.weight\", \"roberta.encoder.layer.0.attention.self.key.bias\", \"roberta.encoder.layer.0.attention.self.value.weight\", \"roberta.encoder.layer.0.attention.self.value.bias\", \"roberta.encoder.layer.0.attention.output.dense.weight\", \"roberta.encoder.layer.0.attention.output.dense.bias\", \"roberta.encoder.layer.0.attention.output.LayerNorm.weight\", \"roberta.encoder.layer.0.attention.output.LayerNorm.bias\", \"roberta.encoder.layer.0.intermediate.dense.weight\", \"roberta.encoder.layer.0.intermediate.dense.bias\", \"roberta.encoder.layer.0.output.dense.weight\", \"roberta.encoder.layer.0.output.dense.bias\", \"roberta.encoder.layer.0.output.LayerNorm.weight\", \"roberta.encoder.layer.0.output.LayerNorm.bias\", \"roberta.encoder.layer.1.attention.self.query.weight\", \"roberta.encoder.layer.1.attention.self.query.bias\", \"roberta.encoder.layer.1.attention.self.key.weight\", \"roberta.encoder.layer.1.attention.self.key.bias\", \"roberta.encoder.layer.1.attention.self.value.weight\", \"roberta.encoder.layer.1.attention.self.value.bias\", \"roberta.encoder.layer.1.attention.output.dense.weight\", \"roberta.encoder...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVQmUqNDZBl4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}