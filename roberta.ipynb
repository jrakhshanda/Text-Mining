{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO5K9HiuY3iDkfvXrFtvurz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d20f82323c5e46e980a1c1ae1e7cda5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03c233e4b5b44a7c971577b0c3064bdb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_51fd7450e78b426ea9e648d67e6bcd16",
              "IPY_MODEL_7e326fa327d749ae95d1591d54deb083"
            ]
          }
        },
        "03c233e4b5b44a7c971577b0c3064bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51fd7450e78b426ea9e648d67e6bcd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_559ccaac0fea41a0ae445af95380723e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3ec1b92f3744b709717c25b261b83c1"
          }
        },
        "7e326fa327d749ae95d1591d54deb083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f8cc5e9329742d0b0e05ce6fc11f930",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 501M/501M [00:08&lt;00:00, 61.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8646ec83390409bb4508b5d4e7f02ea"
          }
        },
        "559ccaac0fea41a0ae445af95380723e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3ec1b92f3744b709717c25b261b83c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f8cc5e9329742d0b0e05ce6fc11f930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8646ec83390409bb4508b5d4e7f02ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrakhshanda/Text-Mining/blob/main/roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekcMUspsNFY2",
        "outputId": "ef0ab95c-0683-4745-dc26-1009cf1dbc22"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVGoYWYBP9gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12386ad-cd14-418c-b45a-88ab7e3bd390"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install tokenizers\r\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\r\n",
        "#!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\r\n",
        "#!export XLA_USE_BF16=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.9.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9gYGcLNTTz"
      },
      "source": [
        "import os\r\n",
        "import string\r\n",
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn import model_selection\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from transformers import *\r\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
        "import tokenizers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWxMIPZy5W5T",
        "outputId": "74849768-079c-4b0b-a1b5-a11bda95862f"
      },
      "source": [
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7q1nawUPMqT"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgd78eqmNbzc"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\r\n",
        "class config:\r\n",
        "    TRAIN_BATCH_SIZE = 64\r\n",
        "    VALID_BATCH_SIZE = 32\r\n",
        "    EPOCHS = 4\r\n",
        "    PATH = '/content/drive/MyDrive/RoBERTa_files'\r\n",
        "    TRAINING_FILE = pd.read_csv(PATH+'/train.csv')\r\n",
        "    TEST_FILE =  pd.read_csv(PATH+'/test.csv')\r\n",
        "    MAX_LEN = 192\r\n",
        "    TOKENIZER = ByteLevelBPETokenizer(f\"{PATH}/vocab.json\",\r\n",
        "                                      f\"{PATH}/merges.txt\",\r\n",
        "                                      lowercase=True, add_prefix_space=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOXQ5K2XA-l"
      },
      "source": [
        "# Processing of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-qngeJ3Xahc"
      },
      "source": [
        "def process_data(text, selected_text, sentiment, tokenizer=config.TOKENIZER, max_len=config.MAX_LEN):\r\n",
        "\r\n",
        "    text = \" \" + \" \".join(str(text).lower().split(\" \"))\r\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).lower().split(\" \"))\r\n",
        "\r\n",
        "    len_selected = len(selected_text) - 1\r\n",
        "    idx1 = idx2 = None\r\n",
        "    for idx, letter in enumerate(selected_text):\r\n",
        "        if (text[idx] == selected_text[1]) and (\" \" + text[idx: idx+len_selected] == selected_text):\r\n",
        "            idx1 = idx\r\n",
        "            idx2 = idx1 + len_selected - 1\r\n",
        "            break\r\n",
        "    \r\n",
        "    # making character targets\r\n",
        "    if idx1!=None and idx2!=None:\r\n",
        "        char_targets = [0] * len(text)\r\n",
        "        for i in range(idx1, idx2+1):\r\n",
        "            char_targets[i] = 1\r\n",
        "    else:\r\n",
        "        char_targets = [1] * len(text)\r\n",
        "\r\n",
        "    # encoding using pretrained tokenizer\r\n",
        "    tok_text = tokenizer.encode(text)\r\n",
        "    ids = tok_text.ids\r\n",
        "    attention_mask = tok_text.attention_mask\r\n",
        "    type_ids = tok_text.type_ids\r\n",
        "    offsets = tok_text.offsets\r\n",
        "\r\n",
        "    # getting indexes of tokens containing character in selected_text\r\n",
        "    target_idx = []\r\n",
        "    for i, (offset1, offset2) in enumerate(offsets):\r\n",
        "        if sum(char_targets[offset1: offset2])>0:\r\n",
        "            target_idx.append(i)\r\n",
        "\r\n",
        "    # we just need the indexes of the start and end tokens as we are using \r\n",
        "    # nn. CrossEntropy as loss\r\n",
        "    start_target = target_idx[0]\r\n",
        "    end_target = target_idx[-1]\r\n",
        "\r\n",
        "    # token ids of sentiment as present in our vocab hard coded here\r\n",
        "    sentiment_ids = {\r\n",
        "        'positive':1313,                    # tokenizer.encode('positive').ids\r\n",
        "        'negative':2430,                    # tokenizer.encode('negative').ids\r\n",
        "        'neutral':7974                     # tokenizer.encode('neutral').ids\r\n",
        "    }\r\n",
        "\r\n",
        "    # adding special tokens\r\n",
        "    input_ids = [0] + [sentiment_ids[sentiment]] + [2] + [2] + ids + [2]\r\n",
        "    type_ids = [0, 0, 0, 0] + [0] * (len(ids) + 1)\r\n",
        "    attention_mask = [1] * len(type_ids)\r\n",
        "    offsets = [(0, 0)] * 4 + offsets\r\n",
        "    start_target += 4\r\n",
        "    end_target += 4\r\n",
        "\r\n",
        "    # padding\r\n",
        "    padding_len = max_len - len(ids)\r\n",
        "    if padding_len>0:\r\n",
        "        input_ids = input_ids + [1] * padding_len\r\n",
        "        attention_mask = attention_mask + [0] * padding_len\r\n",
        "        type_ids = type_ids + [0] * padding_len\r\n",
        "        offsets = offsets + [(0, 0)] * padding_len\r\n",
        "\r\n",
        "    return {\r\n",
        "        'ids': torch.tensor(input_ids,dtype=torch.long),\r\n",
        "        'attention_mask': torch.tensor(attention_mask,dtype=torch.long),\r\n",
        "        'token_type_ids':torch.tensor(type_ids,dtype=torch.long),\r\n",
        "        'targets_start': torch.tensor(start_target,dtype=torch.long),\r\n",
        "        'targets_end':  torch.tensor(end_target,dtype=torch.long),\r\n",
        "        'offsets': torch.tensor(offsets,dtype=torch.long),\r\n",
        "        'padding_len': padding_len,\r\n",
        "        'text': text,\r\n",
        "        'selected_text': selected_text,\r\n",
        "        'sentiment': sentiment\r\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW51NizeXrjl"
      },
      "source": [
        "class TextDataset(Dataset):\r\n",
        "    def __init__(self, text, sentiment, selected_text):\r\n",
        "        self.text = text\r\n",
        "        self.sentiment = sentiment\r\n",
        "        self.selected_text = selected_text\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.text)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        # processing data\r\n",
        "        data = process_data(\r\n",
        "            self.text[item], \r\n",
        "            self.selected_text[item], \r\n",
        "            self.sentiment[item]\r\n",
        "        )\r\n",
        "        # returning tensors\r\n",
        "        return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN2ApvivZSEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a89c43-e290-4716-d796-20d658ea386c"
      },
      "source": [
        "#import pdb\r\n",
        "#pdb.set_trace()\r\n",
        "df = config.TRAINING_FILE.reset_index(drop=True)\r\n",
        "if __name__== \"__main__\":\r\n",
        "  dset = TextDataset(text = df.text.values,\r\n",
        "                      selected_text =df.selected_text.values,sentiment = df.sentiment.values)\r\n",
        "  print(dset[5])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'ids': tensor([    0,  7974,     2,     2,  2054,   640,  1401,     4,   417, 15244,\n",
            "          428,  7928,  4469,     4,   175,    73,  9426,   506,   111,   103,\n",
            "        36778, 10242,  3923,    13,     5,   275,   910, 15574,  7900,    15,\n",
            "         6872,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0]), 'targets_start': tensor(4), 'targets_end': tensor(30), 'offsets': tensor([[ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  5],\n",
            "        [ 5,  8],\n",
            "        [ 8, 11],\n",
            "        [11, 12],\n",
            "        [12, 13],\n",
            "        [13, 17],\n",
            "        [17, 18],\n",
            "        [18, 21],\n",
            "        [21, 23],\n",
            "        [23, 24],\n",
            "        [24, 27],\n",
            "        [27, 28],\n",
            "        [28, 30],\n",
            "        [30, 31],\n",
            "        [31, 33],\n",
            "        [33, 38],\n",
            "        [38, 48],\n",
            "        [48, 53],\n",
            "        [53, 57],\n",
            "        [57, 61],\n",
            "        [61, 65],\n",
            "        [65, 70],\n",
            "        [70, 72],\n",
            "        [72, 78],\n",
            "        [78, 84],\n",
            "        [84, 87],\n",
            "        [87, 93],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0],\n",
            "        [ 0,  0]]), 'padding_len': 165, 'text': ' http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth', 'selected_text': ' http://www.dothebouncy.com/smf - some shameless plugging for the best rangers forum on earth', 'sentiment': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwNpnriG8GZv"
      },
      "source": [
        "Now we’ll create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcCUgOU8PRA2"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eadEGBNw76"
      },
      "source": [
        "class TextModel(BertPreTrainedModel):\r\n",
        "    def __init__(self,conf):\r\n",
        "        super(TextModel, self).__init__(conf)\r\n",
        "\r\n",
        "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\",config = conf)\r\n",
        "        self.drop_out = nn.Dropout(0.2)\r\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\r\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\r\n",
        "        # this is to initialize the weights of the matrix that would convert \r\n",
        "        # (batch_size, max_len, 2*768) to (batch_size, max_len, 1) with std=0.02 \r\n",
        "    \r\n",
        "    def forward(self, ids, attention_mask, token_type_ids):\r\n",
        "        _, _, output = self.roberta(ids,\r\n",
        "                                    attention_mask = attention_mask,\r\n",
        "                                    token_type_ids=token_type_ids).to_tuple()\r\n",
        "        # out dim = (12, batch_size, max_len, 768)\r\n",
        "        # 12 denotes the 12 hidden layers of roberta\r\n",
        "\r\n",
        "        output = torch.cat((output[-1], output[-2]), dim=-1)\r\n",
        "        # output dim = (batch_size, max_len, 2*768)\r\n",
        "        output = self.drop_out(output)\r\n",
        "        logits = self.l0(output)\r\n",
        "        # logits dim -> (batch_size, max_len, 2)\r\n",
        "\r\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len, 1)\r\n",
        "\r\n",
        "        start_logits = start_logits.squeeze(-1)\r\n",
        "        end_logits = end_logits.squeeze(-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len)\r\n",
        "\r\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lU4SNuJLsiC"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\r\n",
        "params = list(model.named_parameters())\r\n",
        "print('The RoBERTa model has {:} different named parameters.\\n'.format(len(params)))\r\n",
        "print('==== Embedding Layer ====\\n')\r\n",
        "for p in params[0:5]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "print('\\n==== First Transformer ====\\n')\r\n",
        "for p in params[5:21]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\r\n",
        "print('\\n==== Output Layer ====\\n')\r\n",
        "for p in params[-4:]:\r\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZjMaPSEYTvi"
      },
      "source": [
        "# loss function. Play around with it and see what works best\r\n",
        "def loss_fn(output_start, output_end, targets_start, targets_end,device):\r\n",
        "  loss = nn.CrossEntropyLoss().to(device)\r\n",
        "  l1 = loss(output_start,targets_start)\r\n",
        "  l2 = loss(output_end,targets_end)\r\n",
        "  return l1 + l2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fABYRm3GBTuC"
      },
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n",
        "t0 = time.time()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpBxAHPlVzDB"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\r\n",
        "  total_loss = 0\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for bi, batch in enumerate(data_loader):\r\n",
        "    # getting data\r\n",
        "    ids = batch['ids'].to(device, dtype=torch.long)\r\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "    attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "    targets_start = batch['targets_start'].to(device, dtype=torch.long)\r\n",
        "    targets_end = batch['targets_end'].to(device, dtype=torch.long)\r\n",
        "\r\n",
        "    # zeroing gradients\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # getting outputs\r\n",
        "    output_start, output_end = model(ids,\r\n",
        "                                     attention_mask = attention_mask,\r\n",
        "                                     token_type_ids=token_type_ids)\r\n",
        "    # calulating loss\r\n",
        "    loss = loss_fn(output_start, output_end, targets_start, targets_end,device)\r\n",
        "    total_loss += loss.item()\r\n",
        "    # calculating gradients\r\n",
        "    loss.backward()\r\n",
        "    # updating model parameters\r\n",
        "    optimizer.step()\r\n",
        "    # stepping learning rate scheduler\r\n",
        "    scheduler.step()\r\n",
        "  \r\n",
        "  avg_train_loss = total_loss / len(data_loader)\r\n",
        "  print(\"\")\r\n",
        "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "  print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "  return avg_train_loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI6ea68oYSPq"
      },
      "source": [
        "# jaccard function as mentioned in evaluation section of the contest\r\n",
        "def jaccard_metric(text,\r\n",
        "                   selected_text,\r\n",
        "                   sentiment,\r\n",
        "                   offsets,\r\n",
        "                   start_idx,\r\n",
        "                   end_idx): \r\n",
        "  \r\n",
        "  if end_idx < start_idx:\r\n",
        "    end_idx = start_idx\r\n",
        "    \r\n",
        "  pred  = \"\"\r\n",
        "  for idx in range(start_idx, end_idx + 1):\r\n",
        "    pred += text[offsets[idx][0]: offsets[idx][1]]\r\n",
        "    if (idx+1) < len(offsets) and offsets[idx][1] < offsets[idx+1][0]:\r\n",
        "      pred += \" \"\r\n",
        "\r\n",
        "    if len(text.split()) < 5:\r\n",
        "      pred = text\r\n",
        "    \r\n",
        "    if sentiment=='neutral':\r\n",
        "      pred = text\r\n",
        "\r\n",
        "  a = set(selected_text.lower().split()) \r\n",
        "  b = set(pred.lower().split())\r\n",
        "  c = a.intersection(b)\r\n",
        "  jacc = float(len(c)) / (len(a) + len(b) - len(c))\r\n",
        "  return jacc, pred"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o3V2nFgYHy9"
      },
      "source": [
        "def evaluate_fn(data_loader, model, device, tokenizer=config.TOKENIZER):\r\n",
        "  total_loss = 0\r\n",
        "  predicted_text = []\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  for bi, batch in enumerate(data_loader):\r\n",
        "    # getting data\r\n",
        "    ids = batch['ids'].to(device, dtype=torch.long)\r\n",
        "    token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "    attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "    targets_start = batch['targets_start'].to(device, dtype=torch.long)\r\n",
        "    targets_end = batch['targets_end'].to(device, dtype=torch.long)\r\n",
        "    pad_len = batch['padding_len']\r\n",
        "    selected_text = batch['selected_text']\r\n",
        "    text = batch['text']\r\n",
        "    sentiment = batch['sentiment']\r\n",
        "    offsets = batch[\"offsets\"]\r\n",
        "\r\n",
        "    # getting output\r\n",
        "    output_start, output_end = model(ids,\r\n",
        "                                     attention_mask = attention_mask,\r\n",
        "                                     token_type_ids=token_type_ids)\r\n",
        "\r\n",
        "    output_start = torch.softmax(output_start,dim=1).cpu().detach().numpy()\r\n",
        "    output_end = torch.softmax(output_end,dim=1).cpu().detach().numpy()\r\n",
        "\r\n",
        "    jaccard_similarity = []\r\n",
        "    for px, tweet in enumerate(text):\r\n",
        "      jacc, pred = jaccard_metric(tweet,\r\n",
        "                                  selected_text[px],\r\n",
        "                                  sentiment = sentiment[px],\r\n",
        "                                  offsets = offsets[px],\r\n",
        "                                  start_idx=np.argmax(output_start[px,:]),\r\n",
        "                                  end_idx = np.argmax(output_end[px,:]))\r\n",
        "      predicted_text.append(pred)  \r\n",
        "      jaccard_similarity.append(jacc)\r\n",
        "  \r\n",
        "  print(\"  Average jaccard similarity: {0:.2f}\".format(np.mean(jaccard_similarity)))\r\n",
        "  print(\"  validation took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "  return np.mean(jaccard_similarity), predicted_text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA-d3nRp0mWN",
        "outputId": "80de322a-a8e4-4c0b-8361-286fc1895b72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "d20f82323c5e46e980a1c1ae1e7cda5a",
            "03c233e4b5b44a7c971577b0c3064bdb",
            "51fd7450e78b426ea9e648d67e6bcd16",
            "7e326fa327d749ae95d1591d54deb083",
            "559ccaac0fea41a0ae445af95380723e",
            "f3ec1b92f3744b709717c25b261b83c1",
            "7f8cc5e9329742d0b0e05ce6fc11f930",
            "c8646ec83390409bb4508b5d4e7f02ea"
          ]
        }
      },
      "source": [
        "# reading train.csv\r\n",
        "dfx = config.TRAINING_FILE.head(500)\r\n",
        "dfx.reset_index(drop=True)\r\n",
        "# spliting into training and validation set\r\n",
        "train, valid = model_selection.train_test_split(dfx,\r\n",
        "                                                      test_size=0.1,\r\n",
        "                                                      random_state=42,\r\n",
        "                                                      stratify=dfx.sentiment.values)\r\n",
        "\r\n",
        "# using TextDataset function as coded above\r\n",
        "train_dataset = TextDataset(text=train.text.values,\r\n",
        "                            sentiment=train.sentiment.values,\r\n",
        "                            selected_text=train.selected_text.values)\r\n",
        "\r\n",
        "valid_dataset = TextDataset(text=valid.text.values,\r\n",
        "                            sentiment=valid.sentiment.values,\r\n",
        "                            selected_text=valid.selected_text.values)\r\n",
        "\r\n",
        "# making pytorch dataloaders\r\n",
        "train_dataloader = DataLoader(train_dataset,\r\n",
        "                              batch_size=config.TRAIN_BATCH_SIZE)\r\n",
        "\r\n",
        "valid_dataloader = DataLoader(valid_dataset,\r\n",
        "                              batch_size=config.VALID_BATCH_SIZE)\r\n",
        "\r\n",
        "conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "model = TextModel(conf)\r\n",
        "model.to(device)\r\n",
        "    \r\n",
        "# explicitly going through model parameters and removing weight decay\r\n",
        "# from a few layers \r\n",
        "param_optimizer = list(model.named_parameters())\r\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\r\n",
        "optimizer_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay':1e-5},\r\n",
        "                        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0},\r\n",
        "                        ]\r\n",
        "\r\n",
        "# Coding out the optimizer and scheduler\r\n",
        "\r\n",
        "num_train_steps = int(len(train_dataloader) * config.EPOCHS)\r\n",
        "\r\n",
        "optimizer = AdamW(optimizer_parameters, lr=2e-5)\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\r\n",
        "                                            num_warmup_steps=0,\r\n",
        "                                            num_training_steps=num_train_steps)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d20f82323c5e46e980a1c1ae1e7cda5a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=501200538.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91QQaFShYT7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01d30d9-8fbd-4e39-ba8a-51920a0cad5d"
      },
      "source": [
        "import random\r\n",
        "\r\n",
        "# Store the average loss after each epoch so we can plot them.\r\n",
        "\r\n",
        "train_loss = []\r\n",
        "jaccards = []\r\n",
        "best_jaccard = 0\r\n",
        "\r\n",
        "for epoch in range(0, config.EPOCHS):\r\n",
        "  # ========================================\r\n",
        "  #               Training\r\n",
        "  # ========================================\r\n",
        "    \r\n",
        "  # Perform one full pass over the training set.\r\n",
        "  print(\"\")\r\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch+1, config.EPOCHS))\r\n",
        "  print('Training...')\r\n",
        "\r\n",
        "  # Measure how long the training epoch takes.\r\n",
        "  t0 = time.time()\r\n",
        "  \r\n",
        "  avg_train_loss = train_fn(train_dataloader,model,optimizer,device,scheduler)\r\n",
        "  train_loss.append(avg_train_loss)\r\n",
        "  \r\n",
        "  # ========================================\r\n",
        "  #               Validation\r\n",
        "  # ========================================\r\n",
        "  print(\"\")\r\n",
        "  print(\"Running Validation...\")\r\n",
        "  t0 = time.time()\r\n",
        "  jaccard, predictions = evaluate_fn(valid_dataloader, model, device)\r\n",
        "  jaccards.append(jaccard)\r\n",
        "\r\n",
        "  if jaccard > best_jaccard:\r\n",
        "    print('saving model')\r\n",
        "    torch.save(model.state_dict(), config.PATH+'/roberta_modeljcc.pth')\r\n",
        "    best_jaccard = jaccard\r\n",
        "\r\n",
        "  if epoch == 4:\r\n",
        "    print('saving check_point')\r\n",
        "    torch.save(model.state_dict(), config.PATH+\"/roberta_model3.pth\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yfoPC_ZJRQN"
      },
      "source": [
        "conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "model = TextModel(conf)\r\n",
        "model.to(device)\r\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/RoBERTa_files/roberta_model1.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVQmUqNDZBl4"
      },
      "source": [
        "data = {'jaccard':jaccards, \r\n",
        "        'train_loss':train_loss} \r\n",
        "  \r\n",
        "losses = pd.DataFrame(data) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr4QOczm9Y7Y"
      },
      "source": [
        "from google.colab import files\r\n",
        "df.to_csv('losses.csv') \r\n",
        "files.download('losses.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgzEItjo38Pd"
      },
      "source": [
        "total_loss = 0\r\n",
        "model.eval()\r\n",
        "\r\n",
        "for bi, batch in enumerate(data_loader):\r\n",
        "  # getting data\r\n",
        "  ids = batch['ids'].to(device, dtype=torch.long)\r\n",
        "  token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\r\n",
        "  attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\r\n",
        "  targets_start = batch['targets_start'].to(device, dtype=torch.long)\r\n",
        "  targets_end = batch['targets_end'].to(device, dtype=torch.long)\r\n",
        "  selected_text = batch['selected_text']\r\n",
        "  pad_len = batch['padding_len']\r\n",
        "  text = batch['text']\r\n",
        "  sentiment = batch['sentiment']\r\n",
        "  offsets = batch['offsets']\r\n",
        "  # getting output\r\n",
        "  output_start, output_end = model(ids,attention_mask = attention_mask,token_type_ids=token_type_ids)\r\n",
        "    \r\n",
        "    \r\n",
        "  loss = loss_fn(output_start, output_end, targets_start, targets_end,device)\r\n",
        "  total_loss += loss.item()\r\n",
        "\r\n",
        "  output_start = torch.softmax(output_start, dim=1).cpu().detach().numpy()\r\n",
        "  output_end = torch.softmax(output_end, dim=1).cpu().detach().numpy()\r\n",
        "\r\n",
        "  jaccard_similarity = []\r\n",
        "  predicted_text = []\r\n",
        "  for px, tweet in enumerate(text):\r\n",
        "    sel_text = selected_text[px]\r\n",
        "    start_idx = np.argmax(output_start[px,:])\r\n",
        "    end_idx = np.argmax(output_end[px,:])\r\n",
        "    offs = offsets[px]\r\n",
        "    \r\n",
        "    if end_idx < start_idx:\r\n",
        "      end_idx = start_idx\r\n",
        "    \r\n",
        "    pred  = \"\"\r\n",
        "    for idx in range(start_idx, end_idx + 1):\r\n",
        "      pred += tweet[offs[idx][0]: offs[idx][1]]\r\n",
        "      if (idx+1) < len(offs) and offs[idx][1] < offs[idx+1][0]:\r\n",
        "        pred += \" \"\r\n",
        "\r\n",
        "    if len(tweet.split()) < 2:\r\n",
        "      pred = tweet\r\n",
        "    jacc =jaccard_metric(sel_text,pred)\r\n",
        "\r\n",
        "    predicted_text.append(pred)  \r\n",
        "    jaccard_similarity.append(jacc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4VAix679sx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97dd564-a4b8-45b4-9448-7f91dbc07b03"
      },
      "source": [
        "jaccs, predictions = evaluate_fn(train_dataloader,model,device,config.TOKENIZER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Average validation loss: 232.30\n",
            "  Average jaccard similarity: 0.73\n",
            "  validation took: 0:06:41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7xOKpNKSYA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c4ecf2-ba5c-4137-c26c-8efa6ecc9936"
      },
      "source": [
        "jaccs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.726385717457146"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlxDEAjlSITi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}