{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "roberta.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnUccdJvcv3AC6jg4AHxd0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrakhshanda/Text-Mining/blob/main/roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekcMUspsNFY2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVGoYWYBP9gs"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj9gYGcLNTTz"
      },
      "source": [
        "import os\r\n",
        "import string\r\n",
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn import model_selection\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from transformers import *\r\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
        "import tokenizers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7q1nawUPMqT"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgd78eqmNbzc"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\r\n",
        "class config:\r\n",
        "    TRAIN_BATCH_SIZE = 32\r\n",
        "    VALID_BATCH_SIZE = 8\r\n",
        "    EPOCHS = 5\r\n",
        "    PATH = '/content/drive/MyDrive/RoBERTa_files'\r\n",
        "    TRAINING_FILE = pd.read_csv(PATH + '/TRAINING_FILE.csv')\r\n",
        "    TEST_FILE =  pd.read_csv(PATH + '/TEST_FILE.csv')\r\n",
        "    MAX_LEN = 141\r\n",
        "    TOKENIZER = ByteLevelBPETokenizer(f\"{PATH}/vocab.json\",\r\n",
        "                                      f\"{PATH}/merges.txt\",\r\n",
        "                                      lowercase=True, add_prefix_space=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcCUgOU8PRA2"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eadEGBNw76"
      },
      "source": [
        "\r\n",
        "class TweetModel(BertPreTrainedModel):\r\n",
        "    def __init__(self,conf):\r\n",
        "        super(TweetModel, self).__init__(conf)\r\n",
        "\r\n",
        "        self.roberta = RobertaModel.from_pretrained(f\"{config.PATH}/pytorch_model.bin\",\r\n",
        "                                                    config = conf)\r\n",
        "        self.drop_out = nn.Dropout(0.1)\r\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\r\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\r\n",
        "        # this is to initialize the weights of the matrix that would convert \r\n",
        "        # (batch_size, max_len, 2*768) to (batch_size, max_len, 1) with std=0.02 \r\n",
        "    \r\n",
        "    def forward(self, ids, mask, token_type_ids):\r\n",
        "        _, _, out = self.roberta(\r\n",
        "            ids,\r\n",
        "            attention_mask=mask,\r\n",
        "            token_type_ids=token_type_ids\r\n",
        "        )\r\n",
        "        # out dim -> (12, batch_size, max_len, 768)\r\n",
        "        # 12 denotes the 12 hidden layers of roberta\r\n",
        "\r\n",
        "        #out = torch.cat((out[-1], out[-2]), dim=-1)\r\n",
        "        # out dim -> (batch_size, max_len, 2*768)\r\n",
        "        #out = self.drop_out(out)\r\n",
        "        logits = self.l0(out)\r\n",
        "        # logits dim -> (batch_size, max_len, 2)\r\n",
        "\r\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len, 1)\r\n",
        "\r\n",
        "        start_logits = start_logits.squeeze(-1)\r\n",
        "        end_logits = end_logits.squeeze(-1)\r\n",
        "        # start_logits and end_logits dim -> (batch_size, max_len)\r\n",
        "\r\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFOXQ5K2XA-l"
      },
      "source": [
        "# Processing of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-qngeJ3Xahc"
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer=config.TOKENIZER, max_len=config.MAX_LEN):\r\n",
        "    # roberta requires the text to have a prefix space at the beginning\r\n",
        "    tweet = \" \" + \" \".join(str(tweet).split(\" \"))\r\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split(\" \"))\r\n",
        "\r\n",
        "    # getting initial and final index of selected_text within the tweet\r\n",
        "    len_selected = len(selected_text) - 1\r\n",
        "    idx1 = idx2 = None\r\n",
        "    for idx, letter in enumerate(selected_text):\r\n",
        "        if (tweet[idx] == selected_text[1]) and (\" \" + tweet[idx: idx+len_selected] == selected_text):\r\n",
        "            idx1 = idx\r\n",
        "            idx2 = idx1 + len_selected - 1\r\n",
        "            break\r\n",
        "    \r\n",
        "    # making character targets\r\n",
        "    if idx1!=None and idx2!=None:\r\n",
        "        char_targets = [0] * len(tweet)\r\n",
        "        for i in range(idx1, idx2+1):\r\n",
        "            char_targets[i] = 1\r\n",
        "    else:\r\n",
        "        char_targets = [1] * len(tweet)\r\n",
        "\r\n",
        "    # encoding using pretrained tokenizer\r\n",
        "    tok_tweet = tokenizer.encode(tweet)\r\n",
        "    ids = tok_tweet.ids\r\n",
        "    mask = tok_tweet.attention_mask\r\n",
        "    type_ids = tok_tweet.type_ids\r\n",
        "\r\n",
        "    # getting indexes of tokens containing character in selected_text\r\n",
        "    target_idx = []\r\n",
        "    for i, (offset1, offset2) in enumerate(tok_tweet.offsets):\r\n",
        "        if sum(char_targets[offset1: offset2])>0:\r\n",
        "            target_idx.append(i)\r\n",
        "\r\n",
        "    # we just need the indexes of the start and end tokens as we are using \r\n",
        "    # nn. CrossEntropy as loss\r\n",
        "    start_target = target_idx[0]\r\n",
        "    end_target = target_idx[-1]\r\n",
        "\r\n",
        "    # token ids of sentiment as present in our vocab hard coded here\r\n",
        "    sentiment_ids = {\r\n",
        "        'positive': 1313,\r\n",
        "        'negative': 2430,\r\n",
        "        'neutral': 7974\r\n",
        "    }\r\n",
        "\r\n",
        "    # adding special tokens\r\n",
        "    ids = [0] + [sentiment_ids[sentiment]] + [2] + [2] + ids + [2]\r\n",
        "    mask = [1] * len(ids)\r\n",
        "    type_ids = [0] * len(ids)\r\n",
        "    offsets = [(0, 0)] * 4 + tok_tweet.offsets\r\n",
        "    start_target += 4\r\n",
        "    end_target += 4\r\n",
        "\r\n",
        "    # padding\r\n",
        "    padding_len = max_len - len(ids)\r\n",
        "    if padding_len>0:\r\n",
        "        ids = ids + [1] * padding_len\r\n",
        "        mask = mask + [0] * padding_len\r\n",
        "        type_ids = type_ids + [0] * padding_len\r\n",
        "        offsets = offsets + [(0, 0)] * padding_len\r\n",
        "\r\n",
        "    return {\r\n",
        "        'ids': ids,\r\n",
        "        'mask': mask,\r\n",
        "        'token_type_ids': type_ids,\r\n",
        "        'targets_start': start_target,\r\n",
        "        'targets_end': end_target,\r\n",
        "        'orig_tweet': tweet,\r\n",
        "        'orig_selected': selected_text,\r\n",
        "        'sentiment': sentiment,\r\n",
        "        'offsets': offsets,\r\n",
        "        'padding_len': padding_len\r\n",
        "    }"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW51NizeXrjl"
      },
      "source": [
        "class TweetDataset(Dataset):\r\n",
        "    def __init__(self, tweet, sentiment, selected_text):\r\n",
        "        self.tweet = tweet\r\n",
        "        self.sentiment = sentiment\r\n",
        "        self.selected_text = selected_text\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.tweet)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        # processing data\r\n",
        "        data = process_data(\r\n",
        "            self.tweet[item], \r\n",
        "            self.selected_text[item], \r\n",
        "            self.sentiment[item]\r\n",
        "        )\r\n",
        "\r\n",
        "        # returning tensors\r\n",
        "        return {\r\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\r\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\r\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\r\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\r\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\r\n",
        "            'orig_tweet': data[\"orig_tweet\"],\r\n",
        "            'orig_selected': data[\"orig_selected\"],\r\n",
        "            'sentiment': data[\"sentiment\"],\r\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\r\n",
        "            'padding_len': data[\"padding_len\"]\r\n",
        "        }"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpBxAHPlVzDB"
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device, scheduler):\r\n",
        "    model.train()\r\n",
        "    for d in tqdm(data_loader):\r\n",
        "        # getting data\r\n",
        "        ids = d['ids']\r\n",
        "        token_type_ids = d['token_type_ids']\r\n",
        "        mask = d['mask']\r\n",
        "        targets_start = d['targets_start']\r\n",
        "        targets_end = d['targets_end']\r\n",
        "\r\n",
        "        # putting them into gpu\r\n",
        "        ids = ids.to(device, dtype=torch.long)\r\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\r\n",
        "        mask = mask.to(device, dtype=torch.long)\r\n",
        "        targets_start = targets_start.to(device, dtype=torch.float)\r\n",
        "        targets_end = targets_end.to(device, dtype=torch.float)\r\n",
        "\r\n",
        "        # zeroing gradients\r\n",
        "        optimizer.zero_grad()\r\n",
        "        # getting outputs\r\n",
        "        o1, o2 = model(\r\n",
        "            ids=ids,\r\n",
        "            mask=mask,\r\n",
        "            token_type_ids=token_type_ids\r\n",
        "        )\r\n",
        "        # calulating loss\r\n",
        "        loss = loss_fn(o1, o2, targets_start, targets_end)\r\n",
        "        # calculating gradients\r\n",
        "        loss.backward()\r\n",
        "        # updating model parameters\r\n",
        "        optimizer.step()\r\n",
        "        # stepping learning rate scheduler\r\n",
        "        scheduler.step()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o3V2nFgYHy9"
      },
      "source": [
        "def eval_fn(data_loader, model, device, tokenizer=config.TOKENIZER):\r\n",
        "    model.eval()\r\n",
        "    # below array will store the respective data\r\n",
        "    all_ids = []\r\n",
        "    start_idx = []\r\n",
        "    end_idx = []\r\n",
        "    orig_selected = []\r\n",
        "    padding_len = []\r\n",
        "\r\n",
        "    for d in data_loader:\r\n",
        "        # getting data\r\n",
        "        ids = d['ids']\r\n",
        "        token_type_ids = d['token_type_ids']\r\n",
        "        mask = d['mask']\r\n",
        "        selected_text = d['orig_selected']\r\n",
        "        pad_len = d['padding_len']\r\n",
        "\r\n",
        "        # putting them in gpu\r\n",
        "        ids = ids.to(device, dtype=torch.long)\r\n",
        "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\r\n",
        "        mask = mask.to(device, dtype=torch.long)\r\n",
        "\r\n",
        "        # getting output\r\n",
        "        o1, o2 = model(\r\n",
        "            ids=ids,\r\n",
        "            mask=mask,\r\n",
        "            token_type_ids=token_type_ids\r\n",
        "        )\r\n",
        "\r\n",
        "        # adding to array to use latter\r\n",
        "        # also removing stuff from gpu\r\n",
        "        all_ids.append(ids.cpu().detach().numpy())\r\n",
        "        start_idx.append(torch.sigmoid(o1).cpu().detach().numpy())\r\n",
        "        end_idx.append(torch.sigmoid(o2).cpu().detach().numpy())\r\n",
        "        orig_selected.extend(selected_text)\r\n",
        "        padding_len.extend(pad_len)\r\n",
        "\r\n",
        "    # fixing dimensions\r\n",
        "    start_idx = np.vstack(start_idx)\r\n",
        "    end_idx = np.vstack(end_idx)\r\n",
        "    all_ids = np.vstack(all_ids)\r\n",
        "\r\n",
        "    # to store jaccard score to print mean of it latter\r\n",
        "    jaccards = []\r\n",
        "\r\n",
        "    # getting predicted text and calculating jaccard\r\n",
        "    for i in range(0, len(start_idx)):\r\n",
        "        start_logits = start_idx[i][4: -padding_len[i]-1]\r\n",
        "        end_logits = end_idx[i][4: -padding_len[i]-1]\r\n",
        "        this_id = all_ids[i][4: -padding_len[i]-1]\r\n",
        "\r\n",
        "        idx1 = idx2 = None\r\n",
        "        max_sum = 0\r\n",
        "        for ii, s in enumerate(start_logits):\r\n",
        "            for jj, e in enumerate(end_logits):\r\n",
        "                if  s+e > max_sum:\r\n",
        "                    max_sum = s+e\r\n",
        "                    idx1 = ii\r\n",
        "                    idx2 = jj\r\n",
        "\r\n",
        "        this_id = this_id[idx1: idx2+1]\r\n",
        "        predicted_text = tokenizer.decode(this_id, skip_special_tokens=True)\r\n",
        "        predicted_text = predicted_text.strip()\r\n",
        "        sel_text = orig_selected[i].strip()\r\n",
        "\r\n",
        "        jaccards.append(jaccard(predicted_text, sel_text))\r\n",
        "\r\n",
        "    # returning mean jaccard\r\n",
        "    return np.mean(jaccards)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI6ea68oYSPq"
      },
      "source": [
        "# jaccard function as mentioned in evaluation section of the contest\r\n",
        "def jaccard(str1, str2): \r\n",
        "    a = set(str1.lower().split()) \r\n",
        "    b = set(str2.lower().split())\r\n",
        "    c = a.intersection(b)\r\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZjMaPSEYTvi"
      },
      "source": [
        "# loss function. Play around with it and see what works best\r\n",
        "def loss_fn(o1, o2, t1, t2):\r\n",
        "    l1 = nn.CrossEntropyLoss()(o1, t1.long())\r\n",
        "    l2 = nn.CrossEntropyLoss()(o2, t2.long())\r\n",
        "    return l1 + l2"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91QQaFShYT7v"
      },
      "source": [
        "def run():\r\n",
        "    # reading train.csv\r\n",
        "    dfx = config.TRAINING_FILE\r\n",
        "    # spliting into training and validation set\r\n",
        "    df_train, df_valid = model_selection.train_test_split(\r\n",
        "        dfx,\r\n",
        "        test_size=0.1,\r\n",
        "        random_state=42,\r\n",
        "        stratify=dfx.sentiment.values\r\n",
        "    )\r\n",
        "\r\n",
        "    df_train = df_train.reset_index(drop=True)\r\n",
        "    df_valid = df_valid.reset_index(drop=True)\r\n",
        "\r\n",
        "    # using TweetDataset function as coded above\r\n",
        "    train_dataset = TweetDataset(\r\n",
        "        tweet=df_train.text.values,\r\n",
        "        sentiment=df_train.sentiment.values,\r\n",
        "        selected_text=df_train.selected_text.values\r\n",
        "    )\r\n",
        "\r\n",
        "    valid_dataset = TweetDataset(\r\n",
        "        tweet=df_valid.text.values,\r\n",
        "        sentiment=df_valid.sentiment.values,\r\n",
        "        selected_text=df_valid.selected_text.values\r\n",
        "    )\r\n",
        "\r\n",
        "    # making pytorch dataloaders\r\n",
        "    train_data_loader = torch.utils.data.DataLoader(\r\n",
        "        train_dataset,\r\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\r\n",
        "        num_workers=4\r\n",
        "    )\r\n",
        "\r\n",
        "    valid_data_loader = torch.utils.data.DataLoader(\r\n",
        "        valid_dataset,\r\n",
        "        batch_size=config.VALID_BATCH_SIZE,\r\n",
        "        num_workers=1\r\n",
        "    )\r\n",
        "\r\n",
        "    # making a instance of the model and putting it into gpu\r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "    conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "    conf.output_hidden_states = True\r\n",
        "    model = TweetModel(conf)\r\n",
        "    model.to(device)\r\n",
        "    \r\n",
        "    # explicitly going through model parameters and removing weight decay\r\n",
        "    # from a few layers \r\n",
        "    param_optimizer = list(model.named_parameters())\r\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\r\n",
        "    optimizer_parameters = [\r\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\r\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\r\n",
        "    ]\r\n",
        "\r\n",
        "    # Coding out the optimizer and scheduler\r\n",
        "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\r\n",
        "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\r\n",
        "    scheduler = get_linear_schedule_with_warmup(\r\n",
        "        optimizer,\r\n",
        "        num_warmup_steps=0,\r\n",
        "        num_training_steps=num_train_steps\r\n",
        "    )\r\n",
        "\r\n",
        "    model = nn.DataParallel(model)\r\n",
        "\r\n",
        "    # saving model when we have best jaccard\r\n",
        "    best_jaccard = 0\r\n",
        "    for epoch in range(config.EPOCHS):\r\n",
        "        train_fn(train_data_loader, model, optimizer, device, scheduler)\r\n",
        "        jaccard = eval_fn(valid_data_loader, model, device)\r\n",
        "        print(f\"Jaccard Score = {jaccard}\")\r\n",
        "        if jaccard > best_jaccard:\r\n",
        "            torch.save(model.state_dict(), config.MODEL_PATH)\r\n",
        "            best_jaccard = jaccard"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW6sEWiNfHag"
      },
      "source": [
        "import pdb"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W4eh8U6WYUNM",
        "outputId": "ac911b0f-4f53-4425-85e6-ac37764941aa"
      },
      "source": [
        "pdb.set_trace()\r\n",
        "run()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--Return--\n",
            "> <ipython-input-45-89f5a4558633>(1)<module>()->None\n",
            "-> pdb.set_trace()\n",
            "(Pdb) \n",
            "(Pdb) \n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2885)run_code()\n",
            "-> sys.excepthook = old_excepthook\n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2901)run_code()\n",
            "-> outflag = 0\n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2902)run_code()\n",
            "-> return outflag\n",
            "(Pdb) n\n",
            "--Return--\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2902)run_code()->0\n",
            "-> return outflag\n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2819)run_ast_nodes()\n",
            "-> for i, node in enumerate(to_run_exec):\n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2825)run_ast_nodes()\n",
            "-> for i, node in enumerate(to_run_interactive):\n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2826)run_ast_nodes()\n",
            "-> mod = ast.Interactive([node])\n",
            "(Pdb) n\n",
            "> /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2827)run_ast_nodes()\n",
            "-> code = compiler(mod, cell_name, \"single\")\n",
            "(Pdb) l\n",
            "2822 \t                if self.run_code(code, result):\n",
            "2823 \t                    return True\n",
            "2824 \t\n",
            "2825 \t            for i, node in enumerate(to_run_interactive):\n",
            "2826 \t                mod = ast.Interactive([node])\n",
            "2827 ->\t                code = compiler(mod, cell_name, \"single\")\n",
            "2828 \t                if self.run_code(code, result):\n",
            "2829 \t                    return True\n",
            "2830 \t\n",
            "2831 \t            # Flush softspace\n",
            "2832 \t            if softspace(sys.stdout, 0):\n",
            "--KeyboardInterrupt--\n",
            "(Pdb) c\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/773 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-89f5a4558633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-478cab6ef0fb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mbest_jaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mjaccard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Jaccard Score = {jaccard}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-3fcbf381df6f>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         )\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# calulating loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-0f7696e06e18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# out dim -> (batch_size, max_len, 2*768)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#out = self.drop_out(out)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;31m# logits dim -> (batch_size, max_len, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'dim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bh50F2_QtZe"
      },
      "source": [
        "conf = RobertaConfig.from_pretrained(f\"{config.PATH}/config.json\")\r\n",
        "conf.output_hidden_states = True\r\n",
        "model = Model(conf)\r\n",
        "model = nn.DataParallel(model)\r\n",
        "#model.load_state_dict(torch.load(f\"{config.PATH}/pytorch_model.bin/pytorch_model.bin\"))\r\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVQmUqNDZBl4"
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    }
  ]
}